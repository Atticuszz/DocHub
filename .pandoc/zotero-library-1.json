[{"id":"231209031IComMa","accessed":{"date-parts":[["2024",7,10]]},"citation-key":"231209031IComMa","title":"[2312.09031] iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via Comparing and Matching","type":"webpage","URL":"https://arxiv.org/abs/2312.09031"},{"id":"AddonItem","citation-key":"AddonItem","title":"Addon Item","type":"software"},{"id":"AddonItema","citation-key":"AddonItema","title":"Addon Item","type":"software"},{"id":"caiIkdTreeIncrementalKD2021","abstract":"This paper proposes an efficient data structure, ikd-Tree, for dynamic space partition. The ikd-Tree incrementally updates a k-d tree with new coming points only, leading to much lower computation time than existing static k-d trees. Besides point-wise operations, the ikd-Tree supports several features such as box-wise operations and down-sampling that are practically useful in robotic applications. In parallel to the incremental operations (i.e., insert, re-insert, and delete), ikd-Tree actively monitors the tree structure and partially re-balances the tree, which enables efficient nearest point search in later stages. The ikd-Tree is carefully engineered and supports multi-thread parallel computing to maximize the overall efficiency. We validate the ikd-Tree in both theory and practical experiments. On theory level, a complete time complexity analysis is presented to prove the high efficiency. On experiment level, the ikd-Tree is tested on both randomized datasets and real-world LiDAR point data in LiDAR-inertial odometry and mapping application. In all tests, ikd-Tree consumes only 4% of the running time in a static k-d tree.","accessed":{"date-parts":[["2024",5,21]]},"author":[{"family":"Cai","given":"Yixi"},{"family":"Xu","given":"Wei"},{"family":"Zhang","given":"Fu"}],"citation-key":"caiIkdTreeIncrementalKD2021","DOI":"10.48550/arXiv.2102.10808","issued":{"date-parts":[["2021",2,22]]},"number":"arXiv:2102.10808","publisher":"arXiv","source":"arXiv.org","title":"ikd-Tree: An Incremental K-D Tree for Robotic Applications","title-short":"ikd-Tree","type":"article","URL":"http://arxiv.org/abs/2102.10808"},{"id":"choiIterativeKclosestPoint2020","accessed":{"date-parts":[["2024",5,22]]},"author":[{"family":"Choi","given":"Ouk"},{"family":"Park","given":"Min-Gyu"},{"family":"Hwang","given":"Youngbae"}],"citation-key":"choiIterativeKclosestPoint2020","container-title":"Sensors","DOI":"10.3390/s20185331","issue":"18","issued":{"date-parts":[["2020"]]},"page":"5331","publisher":"MDPI","source":"Google Scholar","title":"Iterative K-closest point algorithms for colored point cloud registration","type":"article-journal","URL":"https://www.mdpi.com/1424-8220/20/18/5331","volume":"20"},{"id":"fanInstantSplatUnboundedSparseview2024","abstract":"While novel view synthesis (NVS) from a sparse set of images has advanced significantly in 3D computer vision, it relies on precise initial estimation of camera parameters using Structure-from-Motion (SfM). For instance, the recently developed Gaussian Splatting depends heavily on the accuracy of SfM-derived points and poses. However, SfM processes are time-consuming and often prove unreliable in sparse-view scenarios, where matched features are scarce, leading to accumulated errors and limited generalization capability across datasets. In this study, we introduce a novel and efficient framework to enhance robust NVS from sparse-view images. Our framework, InstantSplat, integrates multi-view stereo(MVS) predictions with point-based representations to construct 3D Gaussians of large-scale scenes from sparse-view data within seconds, addressing the aforementioned performance and efficiency issues by SfM. Specifically, InstantSplat generates densely populated surface points across all training views and determines the initial camera parameters using pixel-alignment. Nonetheless, the MVS points are not globally accurate, and the pixel-wise prediction from all views results in an excessive Gaussian number, yielding a overparameterized scene representation that compromises both training speed and accuracy. To address this issue, we employ a grid-based, confidence-aware Farthest Point Sampling to strategically position point primitives at representative locations in parallel. Next, we enhance pose accuracy and tune scene parameters through a gradient-based joint optimization framework from self-supervision. By employing this simplified framework, InstantSplat achieves a substantial reduction in training time, from hours to mere seconds, and demonstrates robust performance across various numbers of views in diverse datasets.","accessed":{"date-parts":[["2024",7,10]]},"author":[{"family":"Fan","given":"Zhiwen"},{"family":"Cong","given":"Wenyan"},{"family":"Wen","given":"Kairun"},{"family":"Wang","given":"Kevin"},{"family":"Zhang","given":"Jian"},{"family":"Ding","given":"Xinghao"},{"family":"Xu","given":"Danfei"},{"family":"Ivanovic","given":"Boris"},{"family":"Pavone","given":"Marco"},{"family":"Pavlakos","given":"Georgios"},{"family":"Wang","given":"Zhangyang"},{"family":"Wang","given":"Yue"}],"citation-key":"fanInstantSplatUnboundedSparseview2024","DOI":"10.48550/arXiv.2403.20309","issued":{"date-parts":[["2024",6,30]]},"number":"arXiv:2403.20309","publisher":"arXiv","source":"arXiv.org","title":"InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds","title-short":"InstantSplat","type":"article","URL":"http://arxiv.org/abs/2403.20309"},{"id":"guptaNDT6DColor2023","abstract":"Abstract\n            Registration of point cloud data containing both depth and color information is critical for a variety of applications, including in‐field robotic plant manipulation, crop growth modeling, and autonomous navigation. However, current state‐of‐the‐art registration methods often fail in challenging agricultural field conditions due to factors such as occlusions, plant density, and variable illumination. To address these issues, we propose the NDT‐6D registration method, which is a color‐based variation of the Normal Distribution Transform (NDT) registration approach for point clouds. Our method computes correspondences between pointclouds using both geometric and color information and minimizes the distance between these correspondences using only the three‐dimensional (3D) geometric dimensions. We evaluate the method using the GRAPES3D data set collected with a commercial‐grade RGB‐D sensor mounted on a mobile platform in a vineyard. Results show that registration methods that only rely on depth information fail to provide quality registration for the tested data set. The proposed color‐based variation outperforms state‐of‐the‐art methods with a root mean square error (RMSE) of 1.1–1.6 cm for NDT‐6D compared with 1.1–2.3 cm for other color‐information‐based methods and 1.2–13.7 cm for noncolor‐information‐based methods. The proposed method is shown to be robust against noises using the TUM RGBD data set by artificially adding noise present in an outdoor scenario. The relative pose error (RPE) increased 14% for our method compared to an increase of 75% for the best‐performing registration method. The obtained average accuracy suggests that the NDT‐6D registration methods can be used for in‐field precision agriculture applications, for example, crop detection, size‐based maturity estimation, and growth modeling.","accessed":{"date-parts":[["2024",5,20]]},"author":[{"family":"Gupta","given":"Himanshu"},{"family":"Lilienthal","given":"Achim J."},{"family":"Andreasson","given":"Henrik"},{"family":"Kurtser","given":"Polina"}],"citation-key":"guptaNDT6DColor2023","container-title":"Journal of Field Robotics","container-title-short":"Journal of Field Robotics","DOI":"10.1002/rob.22194","ISSN":"1556-4959, 1556-4967","issue":"6","issued":{"date-parts":[["2023",9]]},"language":"en","page":"1603-1619","source":"DOI.org (Crossref)","title":"NDT‐6D for color registration in agri‐robotic applications","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/10.1002/rob.22194","volume":"40"},{"id":"hanPCKRFPointCloud2024","accessed":{"date-parts":[["2024",6,3]]},"author":[{"family":"Han","given":"Yiheng"},{"family":"Zhan","given":"Irvin Haozhe"},{"family":"Zeng","given":"Long"},{"family":"Wang","given":"Yu-Ping"},{"family":"Yi","given":"Ran"},{"family":"Yu","given":"Minjing"},{"family":"Lin","given":"Matthieu Gaetan"},{"family":"Sheng","given":"Jenny"},{"family":"Liu","given":"Yong-Jin"}],"citation-key":"hanPCKRFPointCloud2024","container-title":"IEEE Transactions on Visualization and Computer Graphics","DOI":"10.1109/TVCG.2024.3390122","issued":{"date-parts":[["2024"]]},"publisher":"IEEE","source":"Google Scholar","title":"PCKRF: Point Cloud Completion and Keypoint Refinement With Fusion Data for 6D Pose Estimation","title-short":"PCKRF","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/10504632/"},{"id":"hanPointCloudRegistration2024","author":[{"family":"Han","given":"Tianyu"},{"family":"Zhang","given":"Ruijie"},{"family":"Kan","given":"Jiangming"},{"family":"Dong","given":"Ruifang"},{"family":"Zhao","given":"Xixuan"},{"family":"Yao","given":"Shun"}],"citation-key":"hanPointCloudRegistration2024","container-title":"Remote Sensing","DOI":"10.3390/rs16050743","issue":"5","issued":{"date-parts":[["2024"]]},"page":"743","publisher":"MDPI","source":"Google Scholar","title":"A Point Cloud Registration Framework with Color Information Integration","type":"article-journal","volume":"16"},{"id":"haoRobustPointCloud2023","accessed":{"date-parts":[["2024",6,5]]},"author":[{"family":"Hao","given":"Ruidong"},{"family":"Wei","given":"Zhongwei"},{"family":"He","given":"Xu"},{"family":"Zhu","given":"Kaifeng"},{"family":"He","given":"Jiawei"},{"family":"Wang","given":"Jun"},{"family":"Li","given":"Muyu"},{"family":"Zhang","given":"Lei"},{"family":"Lv","given":"Zhuang"},{"family":"Zhang","given":"Xin"}],"citation-key":"haoRobustPointCloud2023","container-title":"Sensors","DOI":"10.3390/s23249837","issue":"24","issued":{"date-parts":[["2023"]]},"page":"9837","publisher":"MDPI","source":"Google Scholar","title":"Robust Point Cloud Registration Network for Complex Conditions","type":"article-journal","URL":"https://www.mdpi.com/1424-8220/23/24/9837","volume":"23"},{"id":"haRGBDGSICPSLAM2024","abstract":"Simultaneous Localization and Mapping (SLAM) with dense representation plays a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR) applications. Recent advancements in dense representation SLAM have highlighted the potential of leveraging neural scene representation and 3D Gaussian representation for high-fidelity spatial representation. In this paper, we propose a novel dense representation SLAM approach with a fusion of Generalized Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast to existing methods, we utilize a single Gaussian map for both tracking and mapping, resulting in mutual benefits. Through the exchange of covariances between tracking and mapping processes with scale alignment techniques, we minimize redundant computations and achieve an efficient system. Additionally, we enhance tracking accuracy and mapping quality through our keyframe selection methods. Experimental results demonstrate the effectiveness of our approach, showing an incredibly fast speed up to 107 FPS (for the entire system) and superior quality of the reconstructed map.","accessed":{"date-parts":[["2024",5,23]]},"author":[{"family":"Ha","given":"Seongbo"},{"family":"Yeon","given":"Jiung"},{"family":"Yu","given":"Hyeonwoo"}],"citation-key":"haRGBDGSICPSLAM2024","issued":{"date-parts":[["2024",3,22]]},"number":"arXiv:2403.12550","publisher":"arXiv","source":"arXiv.org","title":"RGBD GS-ICP SLAM","type":"article","URL":"http://arxiv.org/abs/2403.12550"},{"id":"heIGICPIntensityGeometry2023","accessed":{"date-parts":[["2024",6,3]]},"author":[{"family":"He","given":"Li"},{"family":"Li","given":"Wen"},{"family":"Guan","given":"Yisheng"},{"family":"Zhang","given":"Hong"}],"citation-key":"heIGICPIntensityGeometry2023","container-title":"IEEE Transactions on Intelligent Vehicles","DOI":"10.1109/TIV.2023.3336376","issued":{"date-parts":[["2023"]]},"publisher":"IEEE","source":"Google Scholar","title":"IGICP: Intensity and Geometry Enhanced LiDAR Odometry","title-short":"IGICP","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/10328702/"},{"id":"hondaGeneralizedLOAMLiDAR2022","accessed":{"date-parts":[["2024",5,22]]},"author":[{"family":"Honda","given":"Kohei"},{"family":"Koide","given":"Kenji"},{"family":"Yokozuka","given":"Masashi"},{"family":"Oishi","given":"Shuji"},{"family":"Banno","given":"Atsuhiko"}],"citation-key":"hondaGeneralizedLOAMLiDAR2022","container-title":"IEEE Robotics and Automation Letters","DOI":"10.1109/LRA.2022.3219022","issue":"4","issued":{"date-parts":[["2022"]]},"page":"12459–12466","publisher":"IEEE","source":"Google Scholar","title":"Generalized LOAM: LiDAR Odometry Estimation With Trainable Local Geometric Features","title-short":"Generalized LOAM","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/9935115/","volume":"7"},{"id":"johnsonRegistrationIntegrationTextured1999","accessed":{"date-parts":[["2024",6,3]]},"author":[{"family":"Johnson","given":"Andrew Edie"},{"family":"Kang","given":"Sing Bing"}],"citation-key":"johnsonRegistrationIntegrationTextured1999","container-title":"Image and vision computing","DOI":"10.1016/S0262-8856(98)00117-6","issue":"2","issued":{"date-parts":[["1999"]]},"page":"135–147","publisher":"Elsevier","source":"Google Scholar","title":"Registration and integration of textured 3D data","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0262885698001176","volume":"17"},{"id":"johnsonRegistrationIntegrationTextured1999a","accessed":{"date-parts":[["2024",6,3]]},"author":[{"family":"Johnson","given":"Andrew Edie"},{"family":"Kang","given":"Sing Bing"}],"citation-key":"johnsonRegistrationIntegrationTextured1999a","container-title":"Image and vision computing","DOI":"10.1016/S0262-8856(98)00117-6","issue":"2","issued":{"date-parts":[["1999"]]},"page":"135–147","publisher":"Elsevier","source":"Google Scholar","title":"Registration and integration of textured 3D data","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0262885698001176","volume":"17"},{"id":"kangPointCloudRegistration2024","accessed":{"date-parts":[["2024",6,5]]},"author":[{"family":"Kang","given":"Chuanli"},{"family":"Geng","given":"Chongming"},{"family":"Lin","given":"Zitao"},{"family":"Zhang","given":"Sai"},{"family":"Zhang","given":"Siyao"},{"family":"Wang","given":"Shiwei"}],"citation-key":"kangPointCloudRegistration2024","container-title":"Sensors","DOI":"10.3390/s24061853","issue":"6","issued":{"date-parts":[["2024"]]},"page":"1853","publisher":"MDPI","source":"Google Scholar","title":"Point Cloud Registration Method Based on Geometric Constraint and Transformation Evaluation","type":"article-journal","URL":"https://www.mdpi.com/1424-8220/24/6/1853","volume":"24"},{"id":"karaevCoTrackerItBetter2023","abstract":"We introduce CoTracker, a transformer-based model that tracks dense points in a frame jointly across a video sequence. This differs from most existing state-of-the-art approaches that track points independently, ignoring their correlation. We show that joint tracking results in a significantly higher tracking accuracy and robustness. We also provide several technical innovations, including the concept of virtual tracks, which allows CoTracker to track 70k points jointly and simultaneously. Furthermore, CoTracker operates causally on short windows (hence, it is suitable for online tasks), but is trained by unrolling the windows across longer video sequences, which enables and significantly improves long-term tracking. We demonstrate qualitatively impressive tracking results, where points can be tracked for a long time even when they are occluded or leave the field of view. Quantitatively, CoTracker outperforms all recent trackers on standard benchmarks, often by a substantial margin.","accessed":{"date-parts":[["2024",6,14]]},"author":[{"family":"Karaev","given":"Nikita"},{"family":"Rocco","given":"Ignacio"},{"family":"Graham","given":"Benjamin"},{"family":"Neverova","given":"Natalia"},{"family":"Vedaldi","given":"Andrea"},{"family":"Rupprecht","given":"Christian"}],"citation-key":"karaevCoTrackerItBetter2023","issued":{"date-parts":[["2023",12,26]]},"number":"arXiv:2307.07635","publisher":"arXiv","source":"arXiv.org","title":"CoTracker: It is Better to Track Together","title-short":"CoTracker","type":"article","URL":"http://arxiv.org/abs/2307.07635"},{"id":"keethaSplaTAMSplatTrack2024","type":"article","abstract":"Dense simultaneous localization and mapping (SLAM) is crucial for robotics and augmented reality applications. However, current methods are often hampered by the non-volumetric or implicit way they represent a scene. This work introduces SplaTAM, an approach that, for the first time, leverages explicit volumetric representations, i.e., 3D Gaussians, to enable high-fidelity reconstruction from a single unposed RGB-D camera, surpassing the capabilities of existing methods. SplaTAM employs a simple online tracking and mapping system tailored to the underlying Gaussian representation. It utilizes a silhouette mask to elegantly capture the presence of scene density. This combination enables several benefits over prior representations, including fast rendering and dense optimization, quickly determining if areas have been previously mapped, and structured map expansion by adding more Gaussians. Extensive experiments show that SplaTAM achieves up to 2x superior performance in camera pose estimation, map construction, and novel-view synthesis over existing methods, paving the way for more immersive high-fidelity SLAM applications.","note":"arXiv:2312.02126 [cs]","number":"arXiv:2312.02126","publisher":"arXiv","source":"arXiv.org","title":"SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM","title-short":"SplaTAM","URL":"http://arxiv.org/abs/2312.02126","author":[{"family":"Keetha","given":"Nikhil"},{"family":"Karhade","given":"Jay"},{"family":"Jatavallabhula","given":"Krishna Murthy"},{"family":"Yang","given":"Gengshan"},{"family":"Scherer","given":"Sebastian"},{"family":"Ramanan","given":"Deva"},{"family":"Luiten","given":"Jonathon"}],"accessed":{"date-parts":[["2024",6,9]]},"issued":{"date-parts":[["2024",4,16]]},"citation-key":"keethaSplaTAMSplatTrack2024","library":"My Library","citekey":"keethaSplaTAMSplatTrack2024"},{"id":"kerbl3dGaussianSplatting2023","type":"article-journal","container-title":"ACM Transactions on Graphics","DOI":"10.1145/3592433","issue":"4","note":"publisher: ACM","page":"1–14","source":"Google Scholar","title":"3d gaussian splatting for real-time radiance field rendering","URL":"https://sgvr.kaist.ac.kr/~sungeui/ICG/Students/[CS482]%203D%20Gaussian%20Splatting%20for%20Real-Time%20Radiance%20Field%20Rendering.pdf","volume":"42","author":[{"family":"Kerbl","given":"Bernhard"},{"family":"Kopanas","given":"Georgios"},{"family":"Leimkühler","given":"Thomas"},{"family":"Drettakis","given":"George"}],"accessed":{"date-parts":[["2024",6,9]]},"issued":{"date-parts":[["2023"]]},"citation-key":"kerbl3dGaussianSplatting2023","library":"My Library","citekey":"kerbl3dGaussianSplatting2023"},{"id":"koideExactPointCloud2023","accessed":{"date-parts":[["2024",6,5]]},"author":[{"family":"Koide","given":"Kenji"},{"family":"Oishi","given":"Shuji"},{"family":"Yokozuka","given":"Masashi"},{"family":"Banno","given":"Atsuhiko"}],"citation-key":"koideExactPointCloud2023","container-title":"2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","DOI":"10.1109/IROS55552.2023.10342403","issued":{"date-parts":[["2023"]]},"page":"9175–9182","publisher":"IEEE","source":"Google Scholar","title":"Exact Point Cloud Downsampling for Fast and Accurate Global Trajectory Optimization","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/10342403/"},{"id":"koideGloballyConsistent3D2021","accessed":{"date-parts":[["2024",5,22]]},"author":[{"family":"Koide","given":"Kenji"},{"family":"Yokozuka","given":"Masashi"},{"family":"Oishi","given":"Shuji"},{"family":"Banno","given":"Atsuhiko"}],"citation-key":"koideGloballyConsistent3D2021","container-title":"IEEE Robotics and Automation Letters","DOI":"10.1109/LRA.2021.3113043","issue":"4","issued":{"date-parts":[["2021"]]},"page":"8591–8598","publisher":"IEEE","source":"Google Scholar","title":"Globally consistent 3D LiDAR mapping with GPU-accelerated GICP matching cost factors","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/9540294/","volume":"6"},{"id":"koideVoxelizedGicpFast2021","accessed":{"date-parts":[["2024",5,22]]},"author":[{"family":"Koide","given":"Kenji"},{"family":"Yokozuka","given":"Masashi"},{"family":"Oishi","given":"Shuji"},{"family":"Banno","given":"Atsuhiko"}],"citation-key":"koideVoxelizedGicpFast2021","container-title":"2021 IEEE International Conference on Robotics and Automation (ICRA)","DOI":"10.1109/ICRA48506.2021.9560835","issued":{"date-parts":[["2021"]]},"page":"11054–11059","publisher":"IEEE","source":"Google Scholar","title":"Voxelized gicp for fast and accurate 3d point cloud registration","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/9560835/"},{"id":"koliosDPPEDensePose2024","abstract":"We present DPPE, a dense pose estimation algorithm that functions over a Plenoxels environment. Recent advances in neural radiance field techniques have shown that it is a powerful tool for environment representation. More recent neural rendering algorithms have significantly improved both training duration and rendering speed. Plenoxels introduced a fully-differentiable radiance field technique that uses Plenoptic volume elements contained in voxels for rendering, offering reduced training times and better rendering accuracy, while also eliminating the neural net component. In this work, we introduce a 6-DoF monocular RGB-only pose estimation procedure for Plenoxels, which seeks to recover the ground truth camera pose after a perturbation. We employ a variation on classical template matching techniques, using stochastic gradient descent to optimize the pose by minimizing errors in re-rendering. In particular, we examine an approach that takes advantage of the rapid rendering speed of Plenoxels to numerically approximate part of the pose gradient, using a central differencing technique. We show that such methods are effective in pose estimation. Finally, we perform ablations over key components of the problem space, with a particular focus on image subsampling and Plenoxel grid resolution. Project website: https://sites.google.com/view/dppe","accessed":{"date-parts":[["2024",6,12]]},"author":[{"family":"Kolios","given":"Christopher"},{"family":"Bahoo","given":"Yeganeh"},{"family":"Saeedi","given":"Sajad"}],"citation-key":"koliosDPPEDensePose2024","issued":{"date-parts":[["2024",3,15]]},"number":"arXiv:2403.10773","publisher":"arXiv","source":"arXiv.org","title":"DPPE: Dense Pose Estimation in a Plenoxels Environment using Gradient Approximation","title-short":"DPPE","type":"article","URL":"http://arxiv.org/abs/2403.10773"},{"id":"kornColorSupportedGeneralizedICP2014","accessed":{"date-parts":[["2024",5,22]]},"author":[{"family":"Korn","given":"Michael"},{"family":"Holzkothen","given":"Martin"},{"family":"Pauli","given":"Josef"}],"citation-key":"kornColorSupportedGeneralizedICP2014","container-title":"2014 International Conference on Computer Vision Theory and Applications (VISAPP)","issued":{"date-parts":[["2014"]]},"page":"592–599","publisher":"IEEE","source":"Google Scholar","title":"Color supported generalized-ICP","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/7295135/","volume":"3"},{"id":"kornColorSupportedGeneralizedICP2014a","accessed":{"date-parts":[["2024",5,20]]},"author":[{"family":"Korn","given":"Michael"},{"family":"Holzkothen","given":"Martin"},{"family":"Pauli","given":"Josef"}],"citation-key":"kornColorSupportedGeneralizedICP2014a","container-title":"2014 International Conference on Computer Vision Theory and Applications (VISAPP)","issued":{"date-parts":[["2014"]]},"page":"592–599","publisher":"IEEE","source":"Google Scholar","title":"Color supported generalized-ICP","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/7295135/","volume":"3"},{"id":"lassnerPulsarEfficientSpherebased2021","accessed":{"date-parts":[["2024",6,17]]},"author":[{"family":"Lassner","given":"Christoph"},{"family":"Zollhofer","given":"Michael"}],"citation-key":"lassnerPulsarEfficientSpherebased2021","container-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"page":"1440–1449","source":"Google Scholar","title":"Pulsar: Efficient sphere-based neural rendering","title-short":"Pulsar","type":"paper-conference","URL":"http://openaccess.thecvf.com/content/CVPR2021/html/Lassner_Pulsar_Efficient_Sphere-Based_Neural_Rendering_CVPR_2021_paper.html"},{"id":"lassnerPulsarEfficientSpherebased2021a","accessed":{"date-parts":[["2024",6,17]]},"author":[{"family":"Lassner","given":"Christoph"},{"family":"Zollhofer","given":"Michael"}],"citation-key":"lassnerPulsarEfficientSpherebased2021a","container-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"page":"1440–1449","source":"Google Scholar","title":"Pulsar: Efficient sphere-based neural rendering","title-short":"Pulsar","type":"paper-conference","URL":"http://openaccess.thecvf.com/content/CVPR2021/html/Lassner_Pulsar_Efficient_Sphere-Based_Neural_Rendering_CVPR_2021_paper.html"},{"id":"leeAccurateVisualSimultaneous2023","accessed":{"date-parts":[["2024",5,22]]},"author":[{"family":"Lee","given":"Yangwoo"},{"family":"Kim","given":"Minsoo"},{"family":"Ahn","given":"Joonwoo"},{"family":"Park","given":"Jaeheung"}],"citation-key":"leeAccurateVisualSimultaneous2023","container-title":"Sensors","DOI":"10.3390/s23187947","issue":"18","issued":{"date-parts":[["2023"]]},"page":"7947","publisher":"MDPI","source":"Google Scholar","title":"Accurate Visual Simultaneous Localization and Mapping (SLAM) against Around View Monitor (AVM) Distortion Error Using Weighted Generalized Iterative Closest Point (GICP)","type":"article-journal","URL":"https://www.mdpi.com/1424-8220/23/18/7947","volume":"23"},{"id":"liuGeneticAlgorithmbasedOptimization2022","author":[{"family":"Liu","given":"Dongsheng"},{"family":"Hong","given":"Deyan"},{"family":"Wang","given":"Siting"},{"family":"Chen","given":"Yahui"}],"citation-key":"liuGeneticAlgorithmbasedOptimization2022","container-title":"Frontiers in Bioengineering and Biotechnology","issued":{"date-parts":[["2022"]]},"page":"923736","publisher":"Frontiers","source":"Google Scholar","title":"Genetic algorithm-based optimization for color point cloud registration","type":"article-journal","volume":"10"},{"id":"liuHierarchicalOptimization3D2020","accessed":{"date-parts":[["2024",6,5]]},"author":[{"family":"Liu","given":"Huikai"},{"family":"Zhang","given":"Yue"},{"family":"Lei","given":"Linjian"},{"family":"Xie","given":"Hui"},{"family":"Li","given":"Yan"},{"family":"Sun","given":"Shengli"}],"citation-key":"liuHierarchicalOptimization3D2020","container-title":"Sensors","DOI":"10.3390/s20236999","issue":"23","issued":{"date-parts":[["2020"]]},"page":"6999","publisher":"MDPI","source":"Google Scholar","title":"Hierarchical optimization of 3D point cloud registration","type":"article-journal","URL":"https://www.mdpi.com/1424-8220/20/23/6999","volume":"20"},{"id":"liuHierarchicalOptimization3D2020a","author":[{"family":"Liu","given":"Huikai"},{"family":"Zhang","given":"Yue"},{"family":"Lei","given":"Linjian"},{"family":"Xie","given":"Hui"},{"family":"Li","given":"Yan"},{"family":"Sun","given":"Shengli"}],"citation-key":"liuHierarchicalOptimization3D2020a","container-title":"Sensors","DOI":"10.3390/s20236999","issue":"23","issued":{"date-parts":[["2020"]]},"page":"6999","publisher":"MDPI","source":"Google Scholar","title":"Hierarchical optimization of 3D point cloud registration","type":"article-journal","volume":"20"},{"id":"lyuDynamicDownsamplingAlgorithm2024","accessed":{"date-parts":[["2024",6,5]]},"author":[{"family":"Lyu","given":"Wenqi"},{"family":"Ke","given":"Wei"},{"family":"Sheng","given":"Hao"},{"family":"Ma","given":"Xiao"},{"family":"Zhang","given":"Huayun"}],"citation-key":"lyuDynamicDownsamplingAlgorithm2024","container-title":"Applied Sciences","DOI":"10.3390/app14083160","issue":"8","issued":{"date-parts":[["2024"]]},"page":"3160","publisher":"MDPI","source":"Google Scholar","title":"Dynamic Downsampling Algorithm for 3D Point Cloud Map Based on Voxel Filtering","type":"article-journal","URL":"https://www.mdpi.com/2076-3417/14/8/3160","volume":"14"},{"id":"menColorPointCloud2011","accessed":{"date-parts":[["2024",5,25]]},"author":[{"family":"Men","given":"Hao"},{"family":"Gebre","given":"Biruk"},{"family":"Pochiraju","given":"Kishore"}],"citation-key":"menColorPointCloud2011","container-title":"2011 IEEE International Conference on Robotics and Automation","DOI":"10.1109/ICRA.2011.5980407","issued":{"date-parts":[["2011"]]},"page":"1511–1516","publisher":"IEEE","source":"Google Scholar","title":"Color point cloud registration with 4D ICP algorithm","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/5980407/"},{"id":"namMapbasedMobileRobot2017","author":[{"family":"Nam","given":"Tae Hyeon"},{"family":"Shim","given":"Jae Hong"},{"family":"Cho","given":"Young Im"}],"citation-key":"namMapbasedMobileRobot2017","container-title":"Sensors","DOI":"10.3390/s17122730","issue":"12","issued":{"date-parts":[["2017"]]},"page":"2730","publisher":"MDPI","source":"Google Scholar","title":"A 2.5 D map-based mobile robot localization via cooperation of aerial and ground robots","type":"article-journal","volume":"17"},{"id":"parkColoredPointCloud2017","accessed":{"date-parts":[["2024",5,23]]},"author":[{"family":"Park","given":"Jaesik"},{"family":"Zhou","given":"Qian-Yi"},{"family":"Koltun","given":"Vladlen"}],"citation-key":"parkColoredPointCloud2017","container-title":"Proceedings of the IEEE international conference on computer vision","issued":{"date-parts":[["2017"]]},"page":"143–152","source":"Google Scholar","title":"Colored point cloud registration revisited","type":"paper-conference","URL":"http://openaccess.thecvf.com/content_iccv_2017/html/Park_Colored_Point_Cloud_ICCV_2017_paper.html"},{"id":"raviAccelerating3DDeep2020a","abstract":"Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning.","accessed":{"date-parts":[["2024",6,17]]},"author":[{"family":"Ravi","given":"Nikhila"},{"family":"Reizenstein","given":"Jeremy"},{"family":"Novotny","given":"David"},{"family":"Gordon","given":"Taylor"},{"family":"Lo","given":"Wan-Yen"},{"family":"Johnson","given":"Justin"},{"family":"Gkioxari","given":"Georgia"}],"citation-key":"raviAccelerating3DDeep2020a","issued":{"date-parts":[["2020",7,16]]},"number":"arXiv:2007.08501","publisher":"arXiv","source":"arXiv.org","title":"Accelerating 3D Deep Learning with PyTorch3D","type":"article","URL":"http://arxiv.org/abs/2007.08501"},{"id":"renColorPointCloud2021","author":[{"family":"Ren","given":"Siyu"},{"family":"Chen","given":"Xiaodong"},{"family":"Cai","given":"Huaiyu"},{"family":"Wang","given":"Yi"},{"family":"Liang","given":"Haitao"},{"family":"Li","given":"Haotian"}],"citation-key":"renColorPointCloud2021","container-title":"Applied sciences","DOI":"10.3390/app11125431","issue":"12","issued":{"date-parts":[["2021"]]},"page":"5431","publisher":"MDPI","source":"Google Scholar","title":"Color point cloud registration algorithm based on hue","type":"article-journal","volume":"11"},{"id":"renRobustGICPBased3D2019","abstract":"Unmanned mining is one of the most effective methods to solve mine safety and low efficiency. However, it is the key to accurate localization and mapping for underground mining environment. A novel graph simultaneous localization and mapping (SLAM) optimization method is proposed, which is based on Generalized Iterative Closest Point (GICP) three-dimensional (3D) point cloud registration between consecutive frames, between consecutive key frames and between loop frames, and is constrained by roadway plane and loop. GICP-based 3D point cloud registration between consecutive frames and consecutive key frames is first combined to optimize laser odometer constraints without other sensors such as inertial measurement unit (IMU). According to the characteristics of the roadway, the innovative extraction of the roadway plane as the node constraint of pose graph SLAM, in addition to automatic removing the noise point cloud to further improve the consistency of the underground roadway map. A lightweight and efficient loop detection and optimization based on rules and GICP is designed. Finally, the proposed method was evaluated in four scenes (such as the underground mine laboratory), and compared with the existing 3D laser SLAM method (such as Lidar Odometry and Mapping (LOAM)). The results show that the algorithm could realize low drift localization and point cloud map construction. This method provides technical support for localization and navigation of underground mining environment.","accessed":{"date-parts":[["2024",5,22]]},"author":[{"family":"Ren","given":"Zhuli"},{"family":"Wang","given":"Liguan"},{"family":"Bi","given":"Lin"}],"citation-key":"renRobustGICPBased3D2019","container-title":"Sensors","DOI":"10.3390/s19132915","ISSN":"1424-8220","issue":"13","issued":{"date-parts":[["2019",1]]},"language":"en","license":"http://creativecommons.org/licenses/by/3.0/","number":"13","page":"2915","publisher":"Multidisciplinary Digital Publishing Institute","source":"www.mdpi.com","title":"Robust GICP-Based 3D LiDAR SLAM for Underground Mining Environment","type":"article-journal","URL":"https://www.mdpi.com/1424-8220/19/13/2915","volume":"19"},{"id":"ressSLAMIndoorMapping2024","abstract":"Simultaneous localization and mapping (SLAM), i.e., the reconstruction of the environment represented by a (3D) map and the concurrent pose estimation, has made astonishing progress. Meanwhile, large scale applications aiming at the data collection in complex environments like factory halls or construction sites are becoming feasible. However, in contrast to small scale scenarios with building interiors separated to single rooms, shop floors or construction areas require measures at larger distances in potentially texture less areas under difficult illumination. Pose estimation is further aggravated since no GNSS measures are available as it is usual for such indoor applications. In our work, we realize data collection in a large factory hall by a robot system equipped with four stereo cameras as well as a 3D laser scanner. We apply our state-of-the-art LiDAR and visual SLAM approaches and discuss the respective pros and cons of the different sensor types for trajectory estimation and dense map generation in such an environment. Additionally, dense and accurate depth maps are generated by 3D Gaussian splatting, which we plan to use in the context of our project aiming on the automatic construction and site monitoring.","accessed":{"date-parts":[["2024",5,20]]},"author":[{"family":"Ress","given":"Vincent"},{"family":"Zhang","given":"Wei"},{"family":"Skuddis","given":"David"},{"family":"Haala","given":"Norbert"},{"family":"Soergel","given":"Uwe"}],"citation-key":"ressSLAMIndoorMapping2024","DOI":"10.48550/arXiv.2404.17215","issued":{"date-parts":[["2024",4,26]]},"number":"arXiv:2404.17215","publisher":"arXiv","source":"arXiv.org","title":"SLAM for Indoor Mapping of Wide Area Construction Environments","type":"article","URL":"http://arxiv.org/abs/2404.17215"},{"id":"sahilliogluScaleadaptiveICP2021","accessed":{"date-parts":[["2024",6,3]]},"author":[{"family":"Sahillioğlu","given":"Yusuf"},{"family":"Kavan","given":"Ladislav"}],"citation-key":"sahilliogluScaleadaptiveICP2021","container-title":"Graphical Models","DOI":"10.1016/j.gmod.2021.101113","issued":{"date-parts":[["2021"]]},"page":"101113","publisher":"Elsevier","source":"Google Scholar","title":"Scale-adaptive ICP","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S1524070321000187","volume":"116"},{"id":"sarlinBackFeatureLearning2021","accessed":{"date-parts":[["2024",6,15]]},"author":[{"family":"Sarlin","given":"Paul-Edouard"},{"family":"Unagar","given":"Ajaykumar"},{"family":"Larsson","given":"Mans"},{"family":"Germain","given":"Hugo"},{"family":"Toft","given":"Carl"},{"family":"Larsson","given":"Viktor"},{"family":"Pollefeys","given":"Marc"},{"family":"Lepetit","given":"Vincent"},{"family":"Hammarstrand","given":"Lars"},{"family":"Kahl","given":"Fredrik"}],"citation-key":"sarlinBackFeatureLearning2021","container-title":"Proceedings of the IEEE/CVF conference on computer vision and pattern recognition","issued":{"date-parts":[["2021"]]},"page":"3247–3257","source":"Google Scholar","title":"Back to the feature: Learning robust camera localization from pixels to pose","title-short":"Back to the feature","type":"paper-conference","URL":"http://openaccess.thecvf.com/content/CVPR2021/html/Sarlin_Back_to_the_Feature_Learning_Robust_Camera_Localization_From_Pixels_CVPR_2021_paper.html"},{"id":"schonbergerStructurefrommotionRevisited2016","accessed":{"date-parts":[["2024",6,15]]},"author":[{"family":"Schonberger","given":"Johannes L."},{"family":"Frahm","given":"Jan-Michael"}],"citation-key":"schonbergerStructurefrommotionRevisited2016","container-title":"Proceedings of the IEEE conference on computer vision and pattern recognition","issued":{"date-parts":[["2016"]]},"page":"4104–4113","source":"Google Scholar","title":"Structure-from-motion revisited","type":"paper-conference","URL":"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.html"},{"id":"segalGeneralizedicp2009a","accessed":{"date-parts":[["2024",5,20]]},"author":[{"family":"Segal","given":"Aleksandr"},{"family":"Haehnel","given":"Dirk"},{"family":"Thrun","given":"Sebastian"}],"citation-key":"segalGeneralizedicp2009a","container-title":"Robotics: science and systems","DOI":"10.15607/RSS.2009.V.021","issue":"4","issued":{"date-parts":[["2009"]]},"page":"435","publisher":"Seattle, WA","source":"Google Scholar","title":"Generalized-icp.","type":"paper-conference","URL":"https://direct.mit.edu/books/edited-volume/chapter-pdf/2277340/9780262289801_cau.pdf","volume":"2"},{"id":"smithFlowCamTrainingGeneralizable2023","abstract":"Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datasets. We demonstrate that our method performs robustly on diverse, real-world video, notably on sequences traditionally challenging to optimization-based pose estimation techniques.","accessed":{"date-parts":[["2024",6,14]]},"author":[{"family":"Smith","given":"Cameron"},{"family":"Du","given":"Yilun"},{"family":"Tewari","given":"Ayush"},{"family":"Sitzmann","given":"Vincent"}],"citation-key":"smithFlowCamTrainingGeneralizable2023","issued":{"date-parts":[["2023",5,31]]},"number":"arXiv:2306.00180","publisher":"arXiv","source":"arXiv.org","title":"FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow","title-short":"FlowCam","type":"article","URL":"http://arxiv.org/abs/2306.00180"},{"id":"smithFlowMapHighQualityCamera2024","abstract":"This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence. Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking. Alongside the use of point tracks to encourage long-term geometric consistency, we introduce differentiable re-parameterizations of depth, intrinsics, and pose that are amenable to first-order optimization. We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360-degree trajectories using Gaussian Splatting. Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360-degree novel view synthesis (even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM).","accessed":{"date-parts":[["2024",6,12]]},"author":[{"family":"Smith","given":"Cameron"},{"family":"Charatan","given":"David"},{"family":"Tewari","given":"Ayush"},{"family":"Sitzmann","given":"Vincent"}],"citation-key":"smithFlowMapHighQualityCamera2024","issued":{"date-parts":[["2024",4,23]]},"number":"arXiv:2404.15259","publisher":"arXiv","source":"arXiv.org","title":"FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent","title-short":"FlowMap","type":"article","URL":"http://arxiv.org/abs/2404.15259"},{"id":"sonWGICPDifferentiableWeighted2022","author":[{"family":"Son","given":"Sanghyun"},{"family":"Liang","given":"Jing"},{"family":"Lin","given":"Ming"},{"family":"Manocha","given":"Dinesh"}],"citation-key":"sonWGICPDifferentiableWeighted2022","container-title":"arXiv preprint arXiv:2209.09777","issued":{"date-parts":[["2022"]]},"source":"Google Scholar","title":"WGICP: Differentiable Weighted GICP-Based Lidar Odometry","title-short":"WGICP","type":"article-journal"},{"id":"teedDroidslamDeepVisual2021","accessed":{"date-parts":[["2024",6,15]]},"author":[{"family":"Teed","given":"Zachary"},{"family":"Deng","given":"Jia"}],"citation-key":"teedDroidslamDeepVisual2021","container-title":"Advances in neural information processing systems","issued":{"date-parts":[["2021"]]},"page":"16558–16569","source":"Google Scholar","title":"Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras","title-short":"Droid-slam","type":"article-journal","URL":"https://proceedings.neurips.cc/paper/2021/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html","volume":"34"},{"id":"torrobaPointNetKLDeepInference2020","accessed":{"date-parts":[["2024",6,3]]},"author":[{"family":"Torroba","given":"Ignacio"},{"family":"Sprague","given":"Christopher Iliffe"},{"family":"Bore","given":"Nils"},{"family":"Folkesson","given":"John"}],"citation-key":"torrobaPointNetKLDeepInference2020","container-title":"IEEE Robotics and Automation Letters","DOI":"10.1109/LRA.2020.2988180","issue":"3","issued":{"date-parts":[["2020"]]},"page":"4078–4085","publisher":"IEEE","source":"Google Scholar","title":"PointNetKL: Deep inference for GICP covariance estimation in bathymetric SLAM","title-short":"PointNetKL","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/9072324/","volume":"5"},{"id":"vizzoKissicpDefensePointtopoint2023","accessed":{"date-parts":[["2024",6,3]]},"author":[{"family":"Vizzo","given":"Ignacio"},{"family":"Guadagnino","given":"Tiziano"},{"family":"Mersch","given":"Benedikt"},{"family":"Wiesmann","given":"Louis"},{"family":"Behley","given":"Jens"},{"family":"Stachniss","given":"Cyrill"}],"citation-key":"vizzoKissicpDefensePointtopoint2023","container-title":"IEEE Robotics and Automation Letters","DOI":"10.1109/LRA.2023.3236571","issue":"2","issued":{"date-parts":[["2023"]]},"page":"1029–1036","publisher":"IEEE","source":"Google Scholar","title":"Kiss-icp: In defense of point-to-point icp–simple, accurate, and robust registration if done the right way","title-short":"Kiss-icp","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/10015694/","volume":"8"},{"id":"wangImprovingRGBDPoint2022","accessed":{"date-parts":[["2024",6,5]]},"author":[{"family":"Wang","given":"Ziming"},{"family":"Huo","given":"Xiaoliang"},{"family":"Chen","given":"Zhenghao"},{"family":"Zhang","given":"Jing"},{"family":"Sheng","given":"Lu"},{"family":"Xu","given":"Dong"}],"citation-key":"wangImprovingRGBDPoint2022","container-title":"Computer Vision – ECCV 2022","DOI":"10.1007/978-3-031-19824-3_11","editor":[{"family":"Avidan","given":"Shai"},{"family":"Brostow","given":"Gabriel"},{"family":"Cissé","given":"Moustapha"},{"family":"Farinella","given":"Giovanni Maria"},{"family":"Hassner","given":"Tal"}],"event-place":"Cham","ISBN":"978-3-031-19823-6 978-3-031-19824-3","issued":{"date-parts":[["2022"]]},"language":"en","page":"175-191","publisher":"Springer Nature Switzerland","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Improving RGB-D Point Cloud Registration by Learning Multi-scale Local Linear Transformation","type":"chapter","URL":"https://link.springer.com/10.1007/978-3-031-19824-3_11","volume":"13692"},{"id":"xuGmflowLearningOptical2022","accessed":{"date-parts":[["2024",6,12]]},"author":[{"family":"Xu","given":"Haofei"},{"family":"Zhang","given":"Jing"},{"family":"Cai","given":"Jianfei"},{"family":"Rezatofighi","given":"Hamid"},{"family":"Tao","given":"Dacheng"}],"citation-key":"xuGmflowLearningOptical2022","container-title":"Proceedings of the IEEE/CVF conference on computer vision and pattern recognition","issued":{"date-parts":[["2022"]]},"page":"8121–8130","source":"Google Scholar","title":"Gmflow: Learning optical flow via global matching","title-short":"Gmflow","type":"paper-conference","URL":"http://openaccess.thecvf.com/content/CVPR2022/html/Xu_GMFlow_Learning_Optical_Flow_via_Global_Matching_CVPR_2022_paper.html"},{"id":"yangColorPointCloud2020","accessed":{"date-parts":[["2024",6,3]]},"author":[{"family":"Yang","given":"Yang"},{"family":"Chen","given":"Weile"},{"family":"Wang","given":"Muyi"},{"family":"Zhong","given":"Dexing"},{"family":"Du","given":"Shaoyi"}],"citation-key":"yangColorPointCloud2020","container-title":"Ieee Access","DOI":"10.1109/ACCESS.2020.2963987","issued":{"date-parts":[["2020"]]},"page":"7362–7372","publisher":"IEEE","source":"Google Scholar","title":"Color point cloud registration based on supervoxel correspondence","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/8950119/","volume":"8"},{"id":"yangColorPointCloud2020a","accessed":{"date-parts":[["2024",6,5]]},"author":[{"family":"Yang","given":"Yang"},{"family":"Chen","given":"Weile"},{"family":"Wang","given":"Muyi"},{"family":"Zhong","given":"Dexing"},{"family":"Du","given":"Shaoyi"}],"citation-key":"yangColorPointCloud2020a","container-title":"Ieee Access","DOI":"10.1109/ACCESS.2020.2963987","issued":{"date-parts":[["2020"]]},"page":"7362–7372","publisher":"IEEE","source":"Google Scholar","title":"Color point cloud registration based on supervoxel correspondence","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/8950119/","volume":"8"},{"id":"yangTeaserFastCertifiable2020","accessed":{"date-parts":[["2024",5,21]]},"author":[{"family":"Yang","given":"Heng"},{"family":"Shi","given":"Jingnan"},{"family":"Carlone","given":"Luca"}],"citation-key":"yangTeaserFastCertifiable2020","container-title":"IEEE Transactions on Robotics","DOI":"10.1109/TRO.2020.3033695","issue":"2","issued":{"date-parts":[["2020"]]},"page":"314–333","publisher":"IEEE","source":"Google Scholar","title":"Teaser: Fast and certifiable point cloud registration","title-short":"Teaser","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/9286491/","volume":"37"},{"id":"yeMathematicalSupplementTexttt2023","abstract":"This report provides the mathematical details of the gsplat library, a modular toolbox for efficient differentiable Gaussian splatting, as proposed by Kerbl et al. It provides a self-contained reference for the computations involved in the forward and backward passes of differentiable Gaussian splatting. To facilitate practical usage and development, we provide a user friendly Python API that exposes each component of the forward and backward passes in rasterization at github.com/nerfstudio-project/gsplat .","accessed":{"date-parts":[["2024",6,29]]},"author":[{"family":"Ye","given":"Vickie"},{"family":"Kanazawa","given":"Angjoo"}],"citation-key":"yeMathematicalSupplementTexttt2023","issued":{"date-parts":[["2023",12,4]]},"number":"arXiv:2312.02121","publisher":"arXiv","source":"arXiv.org","title":"Mathematical Supplement for the $\\texttt{gsplat}$ Library","type":"article","URL":"http://arxiv.org/abs/2312.02121"},{"id":"yugayGaussianSLAMPhotorealisticDense2024","type":"article","abstract":"We present a dense simultaneous localization and mapping (SLAM) method that uses 3D Gaussians as a scene representation. Our approach enables interactive-time reconstruction and photo-realistic rendering from real-world single-camera RGBD videos. To this end, we propose a novel effective strategy for seeding new Gaussians for newly explored areas and their effective online optimization that is independent of the scene size and thus scalable to larger scenes. This is achieved by organizing the scene into sub-maps which are independently optimized and do not need to be kept in memory. We further accomplish frame-to-model camera tracking by minimizing photometric and geometric losses between the input and rendered frames. The Gaussian representation allows for high-quality photo-realistic real-time rendering of real-world scenes. Evaluation on synthetic and real-world datasets demonstrates competitive or superior performance in mapping, tracking, and rendering compared to existing neural dense SLAM methods.","note":"arXiv:2312.10070 [cs]","number":"arXiv:2312.10070","publisher":"arXiv","source":"arXiv.org","title":"Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting","title-short":"Gaussian-SLAM","URL":"http://arxiv.org/abs/2312.10070","author":[{"family":"Yugay","given":"Vladimir"},{"family":"Li","given":"Yue"},{"family":"Gevers","given":"Theo"},{"family":"Oswald","given":"Martin R."}],"accessed":{"date-parts":[["2024",6,9]]},"issued":{"date-parts":[["2024",3,22]]},"citation-key":"yugayGaussianSLAMPhotorealisticDense2024","library":"My Library","citekey":"yugayGaussianSLAMPhotorealisticDense2024"},{"id":"zhangRGBDCameraBased2021","accessed":{"date-parts":[["2024",5,22]]},"author":[{"family":"Zhang","given":"He"},{"family":"Jin","given":"Lingqiu"},{"family":"Ye","given":"Cang"}],"citation-key":"zhangRGBDCameraBased2021","container-title":"IEEE/CAA Journal of Automatica Sinica","DOI":"10.1109/JAS.2021.1004084","issue":"8","issued":{"date-parts":[["2021"]]},"page":"1389–1400","publisher":"IEEE","source":"Google Scholar","title":"An RGB-D camera based visual positioning system for assistive navigation by a robotic navigation aid","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/9459587/","volume":"8"},{"id":"zhongRobustRigidRegistration2021","accessed":{"date-parts":[["2024",5,22]]},"author":[{"family":"Zhong","given":"Saishang"},{"family":"Guo","given":"Mingqiang"},{"family":"Lv","given":"Ruina"},{"family":"Chen","given":"Jianguo"},{"family":"Xie","given":"Zhong"},{"family":"Liu","given":"Zheng"}],"citation-key":"zhongRobustRigidRegistration2021","container-title":"Remote Sensing","DOI":"10.3390/rs13234755","issue":"23","issued":{"date-parts":[["2021"]]},"page":"4755","publisher":"MDPI","source":"Google Scholar","title":"A robust rigid registration framework of 3D indoor scene point clouds based on RGB-D information","type":"article-journal","URL":"https://www.mdpi.com/2072-4292/13/23/4755","volume":"13"}]