\documentclass[twocolumn]{article} % 启用双栏排版
\usepackage{arxiv} % 特定于arxiv的样式包，用于格式设置
% packages
\usepackage[utf8]{inputenc} % 允许utf-8输入
\usepackage{fontspec}
\usepackage{url}            % 用于类型设置URL
\usepackage{booktabs}       % 创建专业质量的表格
\usepackage{amsfonts}       % 黑板数学符号
\usepackage{nicefrac}       % 紧凑的分数符号
\usepackage{microtype}      % 微排版
\usepackage{lipsum}         % 生成填充文本
\usepackage{graphicx}       % 图形包
\usepackage{doi}            % 处理DOI
\usepackage{titlesec}       % 调整节标题的间距和格式
\usepackage{setspace}
\setstretch{1.1}
\usepackage[backend=biber, style=ieee, natbib=true]{biblatex} % 使用biblatex处理参考文献，指定使用biber作为后端和IEEE样式
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{colortbl}

\usepackage{adjustbox}


\usepackage{libertinus}



%\usepackage{lmodern}














% 配置链接颜色
\definecolor{authorcolor}{HTML}{800080} % 淡紫色
\definecolor{linkcolor}{HTML}{A6CE39}   % 绿色

% 标题设置
\title{\large\bfseries\textit{GSplatLoc} : Ultra-Precise Pose
Optimization via 3D Gaussian Reprojection}

% 作者设置
\author{
    \href{https://github.com/Atticuszz/GsplatLoc}{\textcolor{linkcolor}{\textit{https://github.com/Atticuszz/GsplatLoc}}}\\[1ex]
    \textcolor{authorcolor}{
                Atticus Zhou,
                Atticus Zhou,
                Atticus Zhou,
                Atticus Zhou    }
}

% PDF元数据
\hypersetup{
    pdftitle={},
    pdfauthor={Atticus Zhou, Atticus Zhou, Atticus Zhou, Atticus Zhou},
}


% % csl for pandoc --citeproc https://github.com/Zettlr/Zettlr/issues/4879
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
\begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
% allow citations to break across lines
\let\@cite@ofmt\@firstofone
% avoid brackets around text for \cite:
\def\@biblabel#1{}
\def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
{\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \setlength{\leftmargin}{\cslhangindent}
  \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
{\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% link


% main
\begin{document}

\twocolumn[
\begin{@twocolumnfalse}
  \maketitle
  \begin{abstract}
    We present GSplatLoc, an innovative pose estimation method for RGB-D
    cameras that employs a volumetric representation of 3D Gaussians.
    This approach facilitates precise pose estimation by minimizing the
    loss based on the reprojection of 3D Gaussians from real depth maps
    captured from the estimated pose. Our method attains rotational
    errors close to zero and translational errors within 0.01mm,
    representing a substantial advancement in pose accuracy over
    existing point cloud registration algorithms, as well as explicit
    volumetric and implicit neural representation-based SLAM methods.
    Comprehensive evaluations demonstrate that GSplatLoc significantly
    improves pose estimation accuracy, which contributes to increased
    robustness and fidelity in real-time 3D scene reconstruction,
    setting a new standard for localization techniques in dense mapping
    SLAM.
  \end{abstract}
  \vspace{1cm}
\end{@twocolumnfalse}
]

\section{Introduction}\label{introduction}

We present GSplatLoc, an innovative pose estimation method for RGB-D
cameras that employs a volumetric representation of 3D Gaussians. This
approach facilitates precise pose estimation by minimizing the loss
based on the reprojection of 3D Gaussians from real depth maps captured
from the estimated pose. Our method attains rotational errors close to
zero and translational errors within 0.01mm, representing a substantial
advancement in pose accuracy over existing point cloud registration
algorithms, as well as explicit volumetric and implicit neural
representation-based SLAM methods. Comprehensive evaluations demonstrate
that GSplatLoc significantly improves pose estimation accuracy, which
contributes to increased robustness and fidelity in real-time 3D scene
reconstruction, setting a new standard for localization techniques in
dense mapping SLAM.

\section{Related Work}\label{related-work}

Accurate visual localization commonly relies on estimating
correspondences between 2D pixel positions and 3D scene coordinates.
Such approaches detect, describe {[}7,49{]}, and match
{[}32,46,48,73,81,96{]} local features, maintain an explicit sparse 3D
representation of the environment, and sometimes leverage image
retrieval {[}33,86{]} to scale to large scenes {[}32,57,70,75,82,87{]}.
Recently, many of these components have been learned with great success
{[}2,23,25,58,60,65,67,71,95{]}, but often independently and not
end-to-end due to the complexity of such systems. Here we introduce a
simpler alternative to feature matching, finally enabling stable
end-to-end training. Our solution can learn more powerful priors than
individual blocks, yet remains highly flexible and interpretable.
End-to-end learning for localization has recently received much
attention. Common approaches encode the scene into a deep network by
regressing from an input image to an absolute pose {[}35,37,59,66,90{]}
or 3D scene coordinates {[}9,13,16,17,80{]}. Pose regression lacks
geometric constraints and thus does not generalize well to novel
viewpoints or appearances {[}76,78{]}, while coordinate regression is
more robust. Both do not scale well due to the limited network capacity
{[}11,82{]} and require for each new scene either costly retraining or
adaptation {[}16,17{]}. ESAC {[}11{]} improves the scalability by
training an ensemble of regressors, each specialized in a scene subset,
but is still significantly less accurate than feature-based methods in
larger environments. Differently, some approaches regress a camera pose
relative to one or more training images {[}5,24,42,97{]}, often after an
explicit retrieval step. They do no memorize the scene geometry and are
thus scene-agnostic, but, similar to absolute regressors, are less
accurate than feature-based methods {[}76,97{]}. Closer to ours, SANet
{[}93{]} takes the scene representation out of the network by regressing
3D coordinates from an input 3D point cloud. Critically, all
top-performing learnable approaches are at least trained per-dataset, if
not per-scene, and are limited to small environments {[}37,80{]}. In
this work we demonstrate the first end-to-end learnable network that
generalizes across scenes, including from outdoor to indoor, and that
delivers performance competitive with complex pipelines on large
real-world datasets, thanks to a differentiable pose solver. Learning
camera pose optimization can be tackled by unrolling the optimizer for a
fixed number of steps {[}21,51,53, 83,91,92{]}, computing implicit
derivatives {[}13,15,18,34,68{]}, or crafting losses to mimic
optimization steps {[}88,89{]}. Multiple works have proposed to learn
components of these optimizers {[}21,51,83{]}, with added complexity and
unclear generalization. Some of these formulations optimize reprojection
errors over sparse points, while others use direct objectives for
(semi-)dense image alignment. The latter are attractive for their
simplicity and accuracy, but usually do not scale well. Like their
classical counterparts {[}26,38{]}, they also suffer from a small basin
of convergence, limiting them to frame tracking. In contrast, PixLoc is
explicitly trained for wide-baseline cross-condition camera pose
estimation from sparse measurements (Figure 2). By focusing on learning
good features, it shows good generalization yet learns sensible data
priors that shape the optimization objective.

\section{Method}\label{method}

\textbf{Overview.} The GSplatLoc method presents an innovative approach
to camera localization, leveraging the differentiable nature of 3D
Gaussian splatting for efficient and accurate pose estimation.

\textbf{Motivation.} Recent advancements in 3D scene representation,
particularly the 3D Gaussian Splatting technique {[}1{]}, have opened
new avenues for efficient and high-quality 3D scene rendering. By
adapting this approach to the task of camera localization, we aim to
exploit its differentiable properties and speed advantages to achieve
robust and real-time pose estimation.

\textbf{Problem formulation.} Our objective is to estimate the 6-DoF
pose \((R, t) \in SE(3)\) of a query depth image \(D_q\), where \(R\) is
the rotation matrix and \(t\) is the translation vector in the camera
coordinate system. Given a 3D representation of the environment in the
form of 3D Gaussians, let \(\mathcal{G} = \{G_i\}_{i=1}^N\) denote a set
of \(N\) 3D Gaussians, and posed reference depth images \(\{D_k\}\),
which together constitute the reference data.

\subsection{Scene Representation}\label{scene-representation}

Building upon the Gaussian splatting method {[}1{]}, we adapt the scene
representation to focus on the differentiable depth rendering process,
which is crucial for our localization task. Our approach utilizes the
efficiency and quality of Gaussian splatting while tailoring it
specifically for depth-based localization.

\textbf{3D Gaussians.} Each Gaussian \(G_i\) is characterized by its 3D
mean \(\boldsymbol{\mu}_i \in \mathbb{R}^3\), 3D covariance matrix
\(\boldsymbol{\Sigma}_i \in \mathbb{R}^{3\times3}\), opacity
\(o_i \in \mathbb{R}\), and scale \(\mathbf{s}_i \in \mathbb{R}^3\). To
represent the orientation of each Gaussian, we use a rotation quaternion
\(\mathbf{q}_i \in \mathbb{R}^4\).

The 3D covariance matrix \(\boldsymbol{\Sigma}_i\) is then parameterized
using \(\mathbf{s}_i\) and \(\mathbf{q}_i\):

\[\boldsymbol{\Sigma}_i = R(\mathbf{q}_i) S(\mathbf{s}_i) S(\mathbf{s}_i)^T R(\mathbf{q}_i)^T\]

where \(R(\mathbf{q}_i)\) is the rotation matrix derived from
\(\mathbf{q}_i\), and \(S(\mathbf{s}_i) = \text{diag}(\mathbf{s}_i)\) is
a diagonal matrix of scales.

\textbf{Projecting 3D to 2D:} To project these 3D Gaussians onto a 2D
image plane, we follow the approach described by {[}1{]}. The projection
of the 3D mean \(\boldsymbol{\mu}_i\) to the 2D image plane is given by:

\[\boldsymbol{\mu}_{I,i} = \pi(P(T_{wc} \boldsymbol{\mu}_{i,\text{homogeneous}}))\]

where \(T_{wc} \in SE(3)\) is the world-to-camera transformation,
\(P \in \mathbb{R}^{4 \times 4}\) is the projection matrix {[}2{]}, and
\(\pi: \mathbb{R}^4 \rightarrow \mathbb{R}^2\) maps to pixel
coordinates.

The 2D covariance
\(\boldsymbol{\Sigma}_{I,i} \in \mathbb{R}^{2\times2}\) of the projected
Gaussian is derived as:

\[\boldsymbol{\Sigma}_{I,i} = J R_{wc} \boldsymbol{\Sigma}_i R_{wc}^T J^T\]

where \(R_{wc}\) represents the rotation component of \(T_{wc}\), and
\(J\) is the affine transform as described by {[}3{]}.

\subsection{Depth Rendering}\label{depth-rendering}

We implement a differential depth rendering process, which is crucial
for our localization method as it allows for gradient computation
throughout the rendering pipeline. This differentiability enables us to
optimize camera poses directly based on rendered depth maps.

\textbf{Compositing Depth:} For depth map generation, we employ a
front-to-back compositing scheme, which allows for accurate depth
estimation and edge alignment. Let \(d_n\) represent the depth value
associated with the \(n\)-th Gaussian, which is the z-coordinate of the
Gaussian's mean in the camera coordinate system. The depth \(D(p)\) at
pixel \(p\) is computed as {[}1{]}:

\[D(p) = \sum_{n \leq N} d_n \cdot \alpha_n \cdot T_n, \quad \text{where } T_n = \prod_{m<n} (1 - \alpha_m)\]

Here, \(\alpha_n\) represents the opacity of the \(n\)-th Gaussian at
pixel \(p\), computed as:

\[\alpha_n = o_n \cdot \exp(-\sigma_n), \quad \sigma_n = \frac{1}{2} \Delta_n^T \boldsymbol{\Sigma}_I^{-1} \Delta_n\]

where \(\Delta_n\) is the offset between the pixel center and the 2D
Gaussian center \(\boldsymbol{\mu}_I\), and \(o_n\) is the opacity
parameter of the Gaussian. \(T_n\) denotes the cumulative transparency
product of all Gaussians preceding \(n\), accounting for the occlusion
effects of previous Gaussians.

\textbf{Scaling Depth.} To ensure consistent representation across the
image, we normalize the depth values. First, we calculate the total
accumulated opacity \(\alpha(p)\) for each pixel:

\[\alpha(p) = \sum_{n \leq N} \alpha_n \cdot T_n\]

The normalized depth \(\text{Norm}_D(p)\) is then defined as:

\[\text{Norm}_D(p) = \frac{D(p)}{\alpha(p)}\]

This normalization process ensures that the depth values are properly
scaled and comparable across different regions of the image, regardless
of the varying densities of Gaussians in the scene.

The differentiable nature of this depth rendering process is key to our
localization method. It allows us to compute gradients with respect to
the Gaussian parameters and camera pose, enabling direct optimization of
the camera pose based on the rendered depth maps. This differentiability
facilitates efficient gradient-based optimization, forming the
foundation for our subsequent localization algorithm.

\subsection{Localization as Image
Alignment}\label{localization-as-image-alignment}

Assuming we have an existing map represented by a set of 3D Gaussians,
our localization task focuses on estimating the 6-DoF pose of a query
depth image \(D_q\) within this map. This process essentially becomes an
image alignment problem between the rendered depth map from our Gaussian
representation and the query depth image.

\textbf{Rotating with Quaternions.} We parameterize the camera pose
using a quaternion \(\mathbf{q}_{cw}\) for rotation and a vector
\(\mathbf{t}_{cw}\) for translation. This choice of parameterization is
particularly advantageous in our differential rendering context.
Quaternions provide a continuous and singularity-free representation of
rotation, which is crucial for gradient-based optimization. Moreover,
their compact four-parameter form aligns well with our differentiable
rendering pipeline, allowing for efficient computation of gradients with
respect to rotation parameters.

\textbf{Loss function.} Our optimization strategy is designed to
leverage the differentiable nature of our depth rendering process. We
define our loss function to incorporate both depth accuracy and edge
alignment:

\[ 
L = \lambda_1 \cdot L_{\text{depth}} + \lambda_2 \cdot L_{\text{contour}} 
\]

where \(L_{\text{depth}}\) represents the L1 loss for depth accuracy,
and \(L_{\text{contour}}\) focuses on the alignment of depth contours or
edges. Specifically:

\[
L_{\text{depth}} = \sum_{i \in M} |D_i^{\text{rendered}} - D_i^{\text{observed}}|
\]

\[
L_{\text{contour}} = \sum_{j \in M} |\nabla D_j^{\text{rendered}} - \nabla D_j^{\text{observed}}|
\]

Here, \(M\) denotes the rendered alpha mask, indicating which pixels are
valid for comparison. Both \(L_{\text{depth}}\) and
\(L_{\text{contour}}\) are computed only over the masked regions.
\(\lambda_1\) and \(\lambda_2\) are weights that balance the two parts
of the loss function, typically set to 0.8 and 0.2 respectively, based
on empirical results.

The contour loss \(L_{\text{contour}}\) is computed using the Sobel
operator {[}4{]}, which effectively captures depth discontinuities and
edges. This additional term in our loss function serves several crucial
purposes. It ensures that depth discontinuities in the rendered image
align well with those in the observed depth image, thereby improving the
overall accuracy of the pose estimation. By explicitly considering edge
information, we preserve important structural features of the scene
during optimization. Furthermore, the contour loss is less sensitive to
absolute depth values and more focused on relative depth changes, making
it robust to global depth scale differences.

The optimization objective can be formulated as:

\[
\min_{\mathbf{q}_{cw}, \mathbf{t}_{cw}} L + \lambda_q \|\mathbf{q}_{cw}\|_2^2 + \lambda_t \|\mathbf{t}_{cw}\|_2^2
\]

where \(\lambda_q\) and \(\lambda_t\) are regularization terms for the
quaternion and translation parameters, respectively.

\textbf{Masking Uncertainty.} The rendered alpha mask plays a crucial
role in our optimization process. It effectively captures the epistemic
uncertainty of our map, allowing us to focus the optimization on
well-represented parts of the scene. By utilizing this mask, we avoid
optimizing based on unreliable or non-existent data, which could
otherwise lead to erroneous pose estimates.

\textbf{Fine-tuning the Engine.} The learning rates are set to
\(5 \times 10^{-4}\) for quaternion optimization and \(10^{-3}\) for
translation optimization, based on empirical results. The weight decay
values, serving as regularization to mitigate overfitting, are set to
\(10^{-3}\) for both quaternion and translation parameters. These
parameters are crucial for balancing the trade-off between convergence
speed and stability in the optimization process.

\subsection{Pipeline}\label{pipeline}

The GSplatLoc method streamlines the localization process by utilizing
only posed reference depth images \(\{D_k\}\) and a query depth image
\(D_q\). Its differentiability in rendering of 3D Gaussians facilitates
efficient and smooth convergence during optimization.

\textbf{Evaluation Scene.} For evaluation consistency, we initialize 3D
Gaussians from point clouds rendered by \(\{D_k\}\). Each point
corresponds to a Gaussian's mean \(\boldsymbol{\mu}_i\). After outlier
filtering, we set opacity \(o_i = 1\) for all Gaussians. The scale
\(\mathbf{s}_i \in \mathbb{R}^3\) is initialized based on local point
density:

\[\mathbf{s}_i = (\sigma_i, \sigma_i, \sigma_i), \text{ where } \sigma_i = \sqrt{\frac{1}{3}\sum_{j=1}^3 d_{ij}^2}\]

Here, \(d_{ij}\) denotes the distance to the \(j\)-th nearest neighbor
of point \(i\), calculated using k-nearest neighbors \((k=4)\). This
isotropic initialization ensures balanced representation of local
geometry. We initially set rotation \(\mathbf{q}_i = (1, 0, 0, 0)\) for
all Gaussians.

To enhance optimization stability, we apply standard Principal Component
Analysis (PCA) for principal axis alignment of the point cloud. This
process involves centering the point cloud at its mean and aligning its
principal axes with the coordinate axes. The PCA-based alignment
normalizes the overall scene orientation, providing a more uniform
starting point for optimization across diverse datasets. This approach
significantly improves the stability of loss reduction during
optimization and facilitates the achievement of lower final loss values,
particularly in the depth loss component of our objective function.

\textbf{Optimization.} We employ the Adam optimizer for both quaternion
and translation optimization, with distinct learning rates and weight
decay values for each. The optimization process benefits from the
real-time rendering capabilities of 3D Gaussian Splatting. Each
iteration of the optimizer is essentially limited only by the speed of
rendering, which is extremely fast due to the efficiency of Gaussian
splatting. This allows for rapid convergence of our pose estimation
algorithm, making it suitable for real-time applications.

\textbf{Convergence.} To determine the convergence of the optimization
process, we implement an early stopping mechanism based on the
stabilization of the total loss. Extensive experimental results indicate
that the total loss typically stabilizes after approximately 100
iterations. We employ a patience mechanism, activated after 100
iterations. If the total loss fails to decrease for a consecutive number
of patience iterations, the optimization loop is terminated. The pose
estimate corresponding to the minimum total loss is subsequently
selected as the optimal pose.

This pipeline effectively combines the efficiency of Gaussian splatting
with a robust optimization strategy, resulting in a fast and accurate
camera localization method.

\section{Evaluation}\label{evaluation}

In this section, we delineate our experimental setup and validate that
the proposed system achieves significant improvements in accuracy.

\subsection{Experimental Setup}\label{experimental-setup}

\textbf{Implementation Details.}~We implemented our SLAM system on a
laptop equipped with a 13th Gen Intel(R) Core(TM) i7-13620H 2.40 GHz
processor, 16GB of RAM, and an NVIDIA RTX 4060 8GB GPU. The optimization
pipeline was implemented using Python with the PyTorch framework, while
custom CUDA kernels were developed for rasterization and backpropagation
operations.

\textbf{Datasets.}~We utilized the Replica dataset {[}5{]} and the
TUM-RGBD dataset {[}6{]} to evaluate our pose estimation accuracy. The
Replica dataset, specifically designed for RGB-D SLAM evaluation,
provides high-quality 3D reconstructions of various indoor scenes. We
employed the publicly available dataset collected by Sucar et al.
{[}7{]}, which offers trajectories from an RGB-D sensor. The Replica
dataset contains challenging purely rotational camera motions. The
TUM-RGBD dataset, widely used in the SLAM field for evaluating tracking
accuracy, represents real-world scenarios and provides precise camera
poses captured by an external motion capture system.

\textbf{Metrics.}~To assess camera pose estimation accuracy, we employed
the average absolute trajectory error (ATE RMSE) and the absolute
angular error (AAE RMSE). In the result tables, we highlight the
\{green\}best\{/green\}, \{yellow\}second\{/yellow\}, and
\{lime\}third\{/lime\} performances.

\textbf{Baselines.}~We primarily compare our method against
state-of-the-art Gaussian-SLAM approaches, including RTG-SLAM{[}8{]},
GS-ICP-SLAM {[}9{]}, and Gaussian-SLAM {[}10{]}. Additionally, we
include the renowned ORB-SLAM3 estimation algorithm {[}11{]} as a
baseline. For a fair comparison, we adapted their tracking methods.
RTG-SLAM employs ICP, GS-ICP-SLAM uses G-ICP {[}12{]}, and Gaussian-SLAM
utilizes Open3D {[}13{]} RGB-D Odometry for pose estimation, which
combines colored point cloud alignment {[}14{]} with an energy-based
approach to visual odometry from RGB-D images {[}15{]}.

\subsection{Localization Evaluation}\label{localization-evaluation}

To mitigate long-term drift accumulation, we focus on evaluating pose
estimation between consecutive frames. For each frame, we initialize the
pose using the ground truth pose of the previous frame.

\begin{table}[htbp]
\renewcommand{\thetable}{\textbf{\arabic{table}}}
\renewcommand{\tablename}{\textbf{Table}}
\centering
\begin{adjustbox}{max width=\columnwidth,max height=!,center}
\begin{tabular}{lccccccccc}
\toprule
\textbf{Methods} & \textbf{Avg.} & \textbf{R0} & \textbf{R1} & \textbf{R2} & \textbf{Of0} & \textbf{Of1} & \textbf{Of2} & \textbf{Of3} & \textbf{Of4}\\
\midrule
RTG-SLAM*[8] & 0.380 & 0.530 & 0.380 & 0.450 & 0.350 & 0.240 & 0.360 & 0.330 & 0.430\\
GS-ICP-SLAM*[9] & \cellcolor{green!30}\textbf{3.090} & \cellcolor{green!30}\textbf{1.370} & \cellcolor{green!30}\textbf{4.700} & \cellcolor{green!30}\textbf{1.470} & \cellcolor{green!30}\textbf{8.480} & \cellcolor{green!30}\textbf{2.040} & \cellcolor{green!30}\textbf{2.580} & \cellcolor{green!30}\textbf{1.110} & \cellcolor{green!30}\textbf{2.940}\\
Gaussian-SLAM*[10] & \cellcolor{yellow!30}1.060 & \cellcolor{yellow!30}0.970 & \cellcolor{yellow!30}1.310 & \cellcolor{yellow!30}1.070 & \cellcolor{yellow!30}0.880 & \cellcolor{yellow!30}1.000 & \cellcolor{yellow!30}1.060 & \cellcolor{yellow!30}1.100 & \cellcolor{yellow!30}1.130\\
 & \cellcolor{lime!50}0.630 & \cellcolor{lime!50}0.710 & \cellcolor{lime!50}0.700 & \cellcolor{lime!50}0.520 & \cellcolor{lime!50}0.570 & \cellcolor{lime!50}0.550 & \cellcolor{lime!50}0.580 & \cellcolor{lime!50}0.720 & \cellcolor{lime!50}0.630\\
\midrule
\textbf{Ours} & 0.016 & 0.015 & 0.013 & 0.021 & 0.011 & 0.009 & 0.018 & 0.020 & 0.019\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[htbp]
\renewcommand{\thetable}{\textbf{\arabic{table}}}
\renewcommand{\tablename}{\textbf{Table}}
\centering
\caption{\textbf{ Replica[5] (AAE RMSE ↓[°])}. This table shows the Absolute Angular Error (AAE) RMSE in degrees for different methods on the Replica dataset. Lower values indicate more accurate rotation estimation.Replica[5] (ATE RMSE ↓[cm]). This table presents the Average Trajectory Error (ATE) Root Mean Square Error (RMSE) in centimeters for various methods on the Replica dataset. Lower values indicate better performance.}
\label{table:_textbf_replica_5_aa}
\begin{adjustbox}{max width=\columnwidth,max height=!,center}
\begin{tabular}{lccccccccc}
\toprule
\textbf{Methods} & \textbf{Avg.} & \textbf{R0} & \textbf{R1} & \textbf{R2} & \textbf{Of0} & \textbf{Of1} & \textbf{Of2} & \textbf{Of3} & \textbf{Of4}\\
\midrule
ICP & \cellcolor{lime!50}0.380 & \cellcolor{lime!50}0.530 & \cellcolor{yellow!30}0.380 & 0.450 & \cellcolor{yellow!30}0.350 & \cellcolor{yellow!30}0.240 & \cellcolor{lime!50}0.360 & \cellcolor{lime!50}0.330 & \cellcolor{yellow!30}0.430\\
Vox-Fusion & 3.090 & 1.370 & 4.700 & 1.470 & 8.480 & 2.040 & 2.580 & 1.110 & 2.940\\
NICE-SLAM & 1.060 & 0.970 & 1.310 & 1.070 & 0.880 & 1.000 & 1.060 & 1.100 & 1.130\\
ESLAM & 0.630 & 0.710 & 0.700 & 0.520 & 0.570 & 0.550 & 0.580 & 0.720 & 0.630\\
Point-SLAM & 0.520 & 0.610 & 0.410 & \cellcolor{lime!50}0.370 & \cellcolor{lime!50}0.380 & 0.480 & 0.540 & 0.690 & 0.720\\
SplaTAM & \cellcolor{yellow!30}0.360 & \cellcolor{yellow!30}0.310 & \cellcolor{lime!50}0.400 & \cellcolor{yellow!30}0.290 & 0.470 & \cellcolor{lime!50}0.270 & \cellcolor{yellow!30}0.290 & \cellcolor{yellow!30}0.320 & \cellcolor{lime!50}0.550\\
\midrule
\textbf{Ours} & \cellcolor{green!30}\textbf{0.009} & \cellcolor{green!30}\textbf{0.007} & \cellcolor{green!30}\textbf{0.008} & \cellcolor{green!30}\textbf{0.010} & \cellcolor{green!30}\textbf{0.009} & \cellcolor{green!30}\textbf{0.009} & \cellcolor{green!30}\textbf{0.011} & \cellcolor{green!30}\textbf{0.009} & \cellcolor{green!30}\textbf{0.011}\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[htbp]
\renewcommand{\thetable}{\textbf{\arabic{table}}}
\renewcommand{\tablename}{\textbf{Table}}
\centering
\caption{\textbf{TUM[6] (ATE RMSE ↓[cm] and AAE RMSE ↓[°])}. These tables present the ATE RMSE in centimeters and AAE RMSE in degrees, respectively, for various methods on the TUM-RGBD dataset. The results demonstrate the performance of different SLAM systems on real-world data.}
\label{table:_textbf_tum_6_ate_rm}
\begin{adjustbox}{max width=\columnwidth,max height=!,center}
\begin{tabular}{lcccccc}
\toprule
\textbf{Methods} & \textbf{Avg.} & \textbf{fr1/desk} & \textbf{fr1/desk2} & \textbf{fr1/room} & \textbf{fr2/xyz} & \textbf{fr3/off.}\\
\midrule
Kintinous & \cellcolor{yellow!30}4.840 & 3.700 & 7.100 & \cellcolor{yellow!30}7.500 & 2.900 & \cellcolor{lime!50}3.000\\
ElasticFusion & 6.910 & \cellcolor{yellow!30}2.530 & 6.830 & 21.490 & \cellcolor{yellow!30}1.170 & \cellcolor{yellow!30}2.520\\
ORB-SLAM2 & \cellcolor{green!30}\textbf{1.980} & \cellcolor{green!30}\textbf{1.600} & \cellcolor{green!30}\textbf{2.200} & \cellcolor{green!30}\textbf{4.700} & \cellcolor{green!30}\textbf{0.400} & \cellcolor{green!30}\textbf{1.000}\\
NICE-SLAM & 15.870 & 4.260 & \cellcolor{lime!50}4.990 & 34.490 & 31.730 & 3.870\\
Vox-Fusion & 11.310 & 3.520 & 6.000 & 19.530 & 1.490 & 26.010\\
Point-SLAM & 8.920 & 4.340 & \cellcolor{yellow!30}4.540 & 30.920 & 1.310 & 3.480\\
\midrule
\textbf{Ours} & \cellcolor{lime!50}5.480 & \cellcolor{lime!50}3.350 & 6.540 & \cellcolor{lime!50}11.130 & \cellcolor{lime!50}1.240 & 5.160\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[htbp]
\renewcommand{\thetable}{\textbf{\arabic{table}}}
\renewcommand{\tablename}{\textbf{Table}}
\centering
\caption{\textbf{TUM[6] (ATE RMSE ↓[cm] and AAE RMSE ↓[°])}. These tables present the ATE RMSE in centimeters and AAE RMSE in degrees, respectively, for various methods on the TUM-RGBD dataset. The results demonstrate the performance of different SLAM systems on real-world data.}
\label{table:_textbf_tum_6_ate_rm}
\begin{adjustbox}{max width=\columnwidth,max height=!,center}
\begin{tabular}{lcccccc}
\toprule
\textbf{Methods} & \textbf{Avg.} & \textbf{fr1/desk} & \textbf{fr1/desk2} & \textbf{fr1/room} & \textbf{fr2/xyz} & \textbf{fr3/off.}\\
\midrule
Kintinous & \cellcolor{yellow!30}4.840 & 3.700 & 7.100 & \cellcolor{yellow!30}7.500 & 2.900 & \cellcolor{lime!50}3.000\\
ElasticFusion & 6.910 & \cellcolor{yellow!30}2.530 & 6.830 & 21.490 & \cellcolor{yellow!30}1.170 & \cellcolor{yellow!30}2.520\\
ORB-SLAM2 & \cellcolor{green!30}\textbf{1.980} & \cellcolor{green!30}\textbf{1.600} & \cellcolor{green!30}\textbf{2.200} & \cellcolor{green!30}\textbf{4.700} & \cellcolor{green!30}\textbf{0.400} & \cellcolor{green!30}\textbf{1.000}\\
NICE-SLAM & 15.870 & 4.260 & \cellcolor{lime!50}4.990 & 34.490 & 31.730 & 3.870\\
Vox-Fusion & 11.310 & 3.520 & 6.000 & 19.530 & 1.490 & 26.010\\
Point-SLAM & 8.920 & 4.340 & \cellcolor{yellow!30}4.540 & 30.920 & 1.310 & 3.480\\
\midrule
\textbf{Ours} & \cellcolor{lime!50}5.480 & \cellcolor{lime!50}3.350 & 6.540 & \cellcolor{lime!50}11.130 & \cellcolor{lime!50}1.240 & 5.160\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\section{Conclusion}\label{conclusion}

We present GSplatLoc, an innovative pose estimation method for RGB-D
cameras that employs a volumetric representation of 3D Gaussians. This
approach facilitates precise pose estimation by minimizing the loss
based on the reprojection of 3D Gaussians from real depth maps captured
from the estimated pose. Our method attains rotational errors close to
zero and translational errors within 0.01mm, representing a substantial
advancement in pose accuracy over existing point cloud registration
algorithms, as well as explicit volumetric and implicit neural
representation-based SLAM methods. Comprehensive evaluations demonstrate
that GSplatLoc significantly improves pose estimation accuracy, which
contributes to increased robustness and fidelity in real-time 3D scene
reconstruction, setting a new standard for localization techniques in
dense mapping SLAM.

\phantomsection\label{refs}
\begin{CSLReferences}{0}{0}
\bibitem[\citeproctext]{ref-kerbl3dGaussianSplatting2023}
\CSLLeftMargin{{[}1{]} }%
\CSLRightInline{B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis,
{``3d gaussian splatting for real-time radiance field rendering,''}
\emph{ACM Transactions on Graphics}, vol. 42, no. 4, pp. 1--14, 2023,
doi: \href{https://doi.org/10.1145/3592433}{10.1145/3592433}.}

\bibitem[\citeproctext]{ref-yeMathematicalSupplementTexttt2023}
\CSLLeftMargin{{[}2{]} }%
\CSLRightInline{V. Ye and A. Kanazawa, {``Mathematical {Supplement} for
the \$\textbackslash texttt\{gsplat\}\$ {Library}.''} Accessed: Jun. 29,
2024. {[}Online{]}. Available: \url{http://arxiv.org/abs/2312.02121}}

\bibitem[\citeproctext]{ref-zwickerEWASplatting2002}
\CSLLeftMargin{{[}3{]} }%
\CSLRightInline{M. Zwicker, H. Pfister, J. Van Baar, and M. Gross,
{``{EWA} splatting,''} \emph{IEEE Transactions on Visualization and
Computer Graphics}, vol. 8, no. 3, pp. 223--238, 2002, doi:
\href{https://doi.org/10.1109/TVCG.2002.1021576}{10.1109/TVCG.2002.1021576}.}

\bibitem[\citeproctext]{ref-kanopoulosDesignImageEdge1988}
\CSLLeftMargin{{[}4{]} }%
\CSLRightInline{N. Kanopoulos, N. Vasanthavada, and R. L. Baker,
{``Design of an image edge detection filter using the {Sobel}
operator,''} \emph{IEEE Journal of solid-state circuits}, vol. 23, no.
2, pp. 358--367, 1988, doi:
\href{https://doi.org/10.1109/4.996}{10.1109/4.996}.}

\bibitem[\citeproctext]{ref-straubReplicaDatasetDigital2019}
\CSLLeftMargin{{[}5{]} }%
\CSLRightInline{J. Straub \emph{et al.}, {``The {Replica Dataset}: {A
Digital Replica} of {Indoor Spaces}.''} Accessed: Aug. 10, 2024.
{[}Online{]}. Available: \url{http://arxiv.org/abs/1906.05797}}

\bibitem[\citeproctext]{ref-sturmBenchmarkEvaluationRGBD2012}
\CSLLeftMargin{{[}6{]} }%
\CSLRightInline{J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D.
Cremers, {``A benchmark for the evaluation of {RGB-D SLAM} systems,''}
in \emph{2012 {IEEE}/{RSJ} international conference on intelligent
robots and systems}, IEEE, 2012, pp. 573--580. doi:
\href{https://doi.org/10.1109/IROS.2012.6385773}{10.1109/IROS.2012.6385773}.}

\bibitem[\citeproctext]{ref-sucarImapImplicitMapping2021}
\CSLLeftMargin{{[}7{]} }%
\CSLRightInline{E. Sucar, S. Liu, J. Ortiz, and A. J. Davison, {``Imap:
{Implicit} mapping and positioning in real-time,''} in \emph{Proceedings
of the {IEEE}/{CVF} international conference on computer vision}, 2021,
pp. 6229--6238. Accessed: Aug. 10, 2024. {[}Online{]}. Available:
\url{http://openaccess.thecvf.com/content/ICCV2021/html/Sucar_iMAP_Implicit_Mapping_and_Positioning_in_Real-Time_ICCV_2021_paper.html}}

\bibitem[\citeproctext]{ref-pengRTGSLAMRealtime3D2024}
\CSLLeftMargin{{[}8{]} }%
\CSLRightInline{Z. Peng \emph{et al.}, {``{RTG-SLAM}: {Real-time 3D
Reconstruction} at {Scale} using {Gaussian Splatting}.''} Accessed: Jul.
16, 2024. {[}Online{]}. Available:
\url{http://arxiv.org/abs/2404.19706}}

\bibitem[\citeproctext]{ref-haRGBDGSICPSLAM2024}
\CSLLeftMargin{{[}9{]} }%
\CSLRightInline{S. Ha, J. Yeon, and H. Yu, {``{RGBD GS-ICP SLAM}.''}
Accessed: May 23, 2024. {[}Online{]}. Available:
\url{http://arxiv.org/abs/2403.12550}}

\bibitem[\citeproctext]{ref-yugayGaussianSLAMPhotorealisticDense2024}
\CSLLeftMargin{{[}10{]} }%
\CSLRightInline{V. Yugay, Y. Li, T. Gevers, and M. R. Oswald,
{``Gaussian-{SLAM}: {Photo-realistic Dense SLAM} with {Gaussian
Splatting}.''} Accessed: Jun. 09, 2024. {[}Online{]}. Available:
\url{http://arxiv.org/abs/2312.10070}}

\bibitem[\citeproctext]{ref-camposOrbslam3AccurateOpensource2021}
\CSLLeftMargin{{[}11{]} }%
\CSLRightInline{C. Campos, R. Elvira, J. J. G. Rodríguez, J. M. Montiel,
and J. D. Tardós, {``Orb-Slam3: {An} accurate open-source library for
visual, visual--inertial, and multimap slam,''} \emph{IEEE Transactions
on Robotics}, vol. 37, no. 6, pp. 1874--1890, 2021, doi:
\href{https://doi.org/10.1109/TRO.2021.3075644}{10.1109/TRO.2021.3075644}.}

\bibitem[\citeproctext]{ref-segalGeneralizedicp2009a}
\CSLLeftMargin{{[}12{]} }%
\CSLRightInline{A. Segal, D. Haehnel, and S. Thrun,
{``Generalized-icp.''} in \emph{Robotics: Science and systems}, Seattle,
WA, 2009, p. 435. doi:
\href{https://doi.org/10.15607/RSS.2009.V.021}{10.15607/RSS.2009.V.021}.}

\bibitem[\citeproctext]{ref-zhouOpen3DModernLibrary2018}
\CSLLeftMargin{{[}13{]} }%
\CSLRightInline{Q.-Y. Zhou, J. Park, and V. Koltun, {``{Open3D}: {A
Modern Library} for {3D Data Processing}.''} Accessed: Aug. 20, 2024.
{[}Online{]}. Available: \url{http://arxiv.org/abs/1801.09847}}

\bibitem[\citeproctext]{ref-parkColoredPointCloud2017}
\CSLLeftMargin{{[}14{]} }%
\CSLRightInline{J. Park, Q.-Y. Zhou, and V. Koltun, {``Colored point
cloud registration revisited,''} in \emph{Proceedings of the {IEEE}
international conference on computer vision}, 2017, pp. 143--152.
Accessed: May 23, 2024. {[}Online{]}. Available:
\url{http://openaccess.thecvf.com/content_iccv_2017/html/Park_Colored_Point_Cloud_ICCV_2017_paper.html}}

\bibitem[\citeproctext]{ref-steinbruckerRealtimeVisualOdometry2011}
\CSLLeftMargin{{[}15{]} }%
\CSLRightInline{F. Steinbrücker, J. Sturm, and D. Cremers, {``Real-time
visual odometry from dense {RGB-D} images,''} in \emph{2011 {IEEE}
international conference on computer vision workshops ({ICCV
Workshops})}, IEEE, 2011, pp. 719--722. doi:
\href{https://doi.org/10.1109/ICCVW.2011.6130321}{10.1109/ICCVW.2011.6130321}.}

\end{CSLReferences}

\end{document}
