{"indexedPages":19,"totalPages":19,"version":"10","text":"sensors\nArticle\nRobust GICP-Based 3D LiDAR SLAM for Underground Mining Environment\nZhuli Ren 1,2,*, Liguan Wang 1,2 and Lin Bi 1,2,* 1 School of Resources and Safety Engineering, Central South University, Changsha 410083, China 2 Digital Mine Research Center, Central South University, Changsha 410083, China * Correspondence: renzhuli@yeah.net (Z.R.); mr.bilin@163.com (L.B.)\nReceived: 15 May 2019; Accepted: 26 June 2019; Published: 1 July 2019\nAbstract: Unmanned mining is one of the most eﬀective methods to solve mine safety and low eﬃciency. However, it is the key to accurate localization and mapping for underground mining environment. A novel graph simultaneous localization and mapping (SLAM) optimization method is proposed, which is based on Generalized Iterative Closest Point (GICP) three-dimensional (3D) point cloud registration between consecutive frames, between consecutive key frames and between loop frames, and is constrained by roadway plane and loop. GICP-based 3D point cloud registration between consecutive frames and consecutive key frames is ﬁrst combined to optimize laser odometer constraints without other sensors such as inertial measurement unit (IMU). According to the characteristics of the roadway, the innovative extraction of the roadway plane as the node constraint of pose graph SLAM, in addition to automatic removing the noise point cloud to further improve the consistency of the underground roadway map. A lightweight and eﬃcient loop detection and optimization based on rules and GICP is designed. Finally, the proposed method was evaluated in four scenes (such as the underground mine laboratory), and compared with the existing 3D laser SLAM method (such as Lidar Odometry and Mapping (LOAM)). The results show that the algorithm could realize low drift localization and point cloud map construction. This method provides technical support for localization and navigation of underground mining environment.\nKeywords: underground mine; SLAM; GICP; graph optimization; roadway plane; loop detection\n\n1. Introduction\nWith the rapid development of ground unmanned driving and the harsh environment of deep resource exploitation, in order to improve the safety status of underground transportation operations and maximize the economic beneﬁts of mining enterprises [1], the unmanned of underground mining environment is an inevitable trend in the future development. The intelligent and precise localization of underground mining environment is the key. However, it is impossible to use GPS signals to locate them in the special environment of underground conﬁned spaces such as underground mines and subways. As early as the early 1990s, low frequency electromagnetic, ultrasonic sensing measurements, and visual beacon navigation were used for underground positioning and continuous tracking [2–4]. Localization technologies based on WiFi, Bluetooth, Radio Frequency IDentiﬁcation (RFID), Ultra Wideband (UWB), and ultrasound are also widely used [5]. However, in the above localization and mapping methods, the corresponding auxiliary localization devices need to be installed in the underground environment. Although the positioning accuracy can be improved, a large amount of devices and maintenance costs are required. At the same time, due to the rough underground environment, it is easy to cause a cumulative error during the localization process. Especially, it often fails to get the pose in the case of rotation, which greatly reduces the safety during the autonomous walking of the trackless mining equipment.\n\nSensors 2019, 19, 2915; doi:10.3390/s19132915\n\nwww.mdpi.com/journal/sensors\n\nSensors 2019, 19, 2915\n\n2 of 19\n\nWith the development of computer performance and related optimization algorithms, simultaneous localization and mapping (SLAM) technology [6] is becoming more and more mature. It provides an important reference for the intelligent localization and mapping of underground metal mines, and then realizes path planning and autonomous navigation of trackless mining equipment. Although SLAM technology has made remarkable progress in the past 30 years, for a closed underground special environment, the roadway surface is uneven and degraded over time, which brings diﬃculties to the application of laser SLAM. At the same time, due to the dust and poor lighting conditions in the underground environment, it causes the feature point extraction to be unstable and fails in the visual SLAM. In general, the solution of laser SLAM has a larger application space in underground mines.\nThe two-dimensional (2D) laser-based SLAM has lower computational requirements and the map can be built in real time in the scan plane. However, it cannot estimate the six-degree-of-freedom pose of a mobile robot in three-dimensional (3D) space on uneven ground. Another disadvantage is that long roadways with high similarity are diﬃcult to match accurately because there are too few features available in one scan plane; Huber and Vandapel [7] used the stop-scan-start method to convert 3D laser information into a uniﬁed world coordinate system, and then constructed a high-precision 3D geological model of the mine roadway. However, high-precision laser scanners are expensive, and it cannot meet the real-time mapping and localization requirements of underground mining environment. Thus, many SLAM systems combine laser scanners with other sensors for greater precision and robustness. Lopez et al. [8] integrated 2D laser, vision, altimeter, and inertial measurement unit (IMU) to improve the accurate acquisition of six degrees of freedom (6DoF) pose estimation of robots in environments where GPS signals cannot be received. Bosse et al. [9] coupled a 2D laser to an inertial measurement unit mounted on a spring. This method is well adapted for strenuous motion, but it is suitable for mapping and is not suitable for robot positioning.\nThe current 3D SLAM method is usually computationally intensive, and it is diﬃcult to achieve real-time operation under limited computing resources [7,10,11]. Since there is no real-time method to reduce cumulative error, the laser-based methods most focuses on the point cloud registration process or it is only used in laser odometers, which result in poor adaptability in large-scale and rough ground environments [12]. Aiming at the shortcomings of traditional algorithms, this paper mainly studies the state estimation and environmental mapping of underground mining environment by using 3D laser, which provides support for intelligent mining of underground mines.\nThere are four contributions in this paper: Firstly, Generalized Iterative Closest Point (GICP)-based 3D point cloud registration between consecutive frames and consecutive key frames is ﬁrst combined to optimize laser odometer constraints, which plays a major role in the unstructured environment. Secondly, a fast point cloud segmentation based on RANdom Sample Consensus (RANSAC) is used to extracts the roadway plane, which serves as a landmark to construct the observation constraint in the graph SLAM optimization. Thirdly, a lightweight and eﬃcient loop detection and optimization based on rules and GICP is designed, which is applied to correct motion drift in pose graph optimization. Fourthly, for the constructed sub-map, automated removal of point cloud noise based on the characteristics of the roadway is ﬁrst proposed, and then the completed point cloud map can be widely applied, such as 3D point cloud modeling and positioning based on the known maps.\nIn addition, our results show that a complete 3D LiDAR graph optimized SLAM framework has wider applicability, and it provides technical support for the practical application of special environments in underground conﬁned spaces such as subways, corridors, tunnel ﬁre-ﬁghting, and civil air defense works.\nThe rest of the paper is organized as follows: Section 2 describes the related work; Section 3 describes the proposed method in detail; Section 4 presents an experiment and analysis on underground mining environment; and in the last section, we present conclusions and recommendations for future work.\n\nSensors 2019, 19, 2915\n\n3 of 19\n\n2. Related Work\nThe SLAM method based on laser has been the cornerstone of mobile robot mapping and navigation research for the past 20 years. At the same time, relying on the constructed 3D map for 6D localization is still a research hotspot. Compared with vision sensors, LiDAR provides measurement information that is more robust, accurate, and noise level stable, and is not sensitive to changes in illumination conditions. Therefore, laser SLAM is the most stable and reliable SLAM solution. Most of the work on 3D lasers is frames matching, which is used to predict the relative transformation between two frames. Laser SLAM can be divided into ﬁlter-based and graph-based optimization according to diﬀerent solution methods. The laser SLAM system framework based on graph optimization is currently popular. It is mainly divided into two parts: front end and back end. The front end completes data association and loop detection, and the back end performs graph optimization.\nLaser scan matching is the most common method for achieving laser SLAM data correlation. It is deﬁned as a set of translation and rotation parameters, so that the aligned two-frame scanning point cloud reaches the maximum overlap. Laser scan matching is divided into three categories: (1) point-based scan matching; (2) feature-based scan matching; and (3) scan matching based on mathematical characteristics.\n2.1. Point-Based Scan Matching\nPoint-based scan matching is performed directly on the raw data points acquired by the scan, and the Iterative Closest/Corresponding Point (ICP) algorithm is proposed by Chen [13] and Besl [14] alone, respectively. It is the most widely used, most researched and currently the most mature algorithm. The diﬀerence is that the former uses the point-to-surface distance as the error metric, while the latter uses the point-to-point distance as the error metric, so it can be recorded as Point to Plane ICP and Plane to Plane ICP. Combined with Point-to-Plane ICP algorithm and Plane to Plane ICP algorithm in a single probability framework, GICP algorithm [15] and its improved algorithm [16,17] is proposed. It has become one of the most eﬀective and robust algorithms in many ICP improved algorithms, especially in indoor and structured scenarios where GICP performs better than standard ICP.\n2.2. Feature-Based Scan Matching\nFeature-based scan matching methods are usually based on ﬂexible features such as normal and curvature, as well as custom feature descriptors. Such as the HSM (Hough Scan Matching) method [18], using the Hough transform to extract line segment features and matching in the Hough domain. By using the Loam (LiDAR odometry and mapping) algorithm [19] and its improved algorithm [20], the LiDAR odometer is realized by matching feature points to edge segments and planes, and excellent results have been achieved in various scenarios.\n2.3. Scan Matching Based on Mathematical Characteristics\nIn addition to point-based scan matching and feature-based scan matching, there are a large class of scan matching methods that use various mathematical properties to characterize scan data and frames pose changes, the most famous of which is based on Normal Distributions Transform (NDT) [21] and its improved algorithm [12].\nInevitably, pose accumulative errors will occur only when pose estimation is considered in adjacent time, and it is impossible to obtain globally consistent trajectories and maps. Graph optimization is an eﬀective method to reduce cumulative errors and is widely used in SLAM back-end optimization [22]. Graph-based SLAM usually uses robot poses and landmarks in the environment as state variables. The nodes in the graph are the variables to be optimized, and the edges are the observation constraints between the two interconnected variables. Bundle adjustment (BA) [23] is a classical optimization method based on visual SLAM. It takes the sensor pose and landmarks in the environment as optimization variables, and reduces the size based on sparse features. Key frame nodes formed by\n\nSensoSresn2s0o1rs9,21091,92, 91195, x FOR PEER REVIEW\n\n44 ooff 1199\n\nfeatures. Key frame nodes formed by the sensor pose are considered in the pose optimization [24],\ntherseebnysosarvpinogseoaprteimcoiznastiidoenretidmienftohrempaonsye foepatiumreizs.ation [24], thereby saving optimization time for\nmanyLofeoapturdeest.ection [25] finds correlation between current data and all historical data. More\nconstLroaionptsdeatreectpiornov[2id5e]dﬁntdosbcuoirlrdelatigolnobbaeltlwyeceonncsuisrtreenntt dmaatappanindgalilnhtihsteorgicralpdhatoap. tMimoirzeactioonns.trLaoinotps\nadreetepcrtoiovnidienddteogbraudileddascgelnoebsalsluycchonasisutnednet rmgraopupnindgroinadthweagysraipshstoilpl tdimiffiizcautlitonan. dLotiompedceotnecstuiomnining\ndeugertaodsemd aslcledniefsfesruenchceassinuntedxetrugrreouand freoaatduwreasy. sThiseswtilolrddiﬃ‘bcaugl’t[a2n6]distima ceocmonmsounmminegthdoudeitnovsimsuaall\ndSLiﬀAeMren, cwehsiicnhteuxsteusrfeeantudrefecaltuusrteesr.inTghetowcoorndst‘bruacgt’ a[2d6]icitsioancaormy manodn dmeetethctodsiminilvairsiutiaelsSbLeAtwMe,ewn htwicho\nuimseasgfeefartaumreesc.luInstGeroinogleto'scCoanrsttorgurcatpahderic[t2io7]n,atrhye abnradndcheteacntdsbimouilnadritmieestbheotdwieseanptpwlioedimtoafgreamfraemaneds.\nIsnubG-mooagplem’satCcahrintoggrtoapchrearte[2l7o]o, pthceonbsrtarnacinhtsa.nMdabgonuunsdsomn e[2th8]odpriospaopsepdlieadn taoppfreaamraencaen-dbassuedb-mlooap\nmaetchhoidngthtaotccrreeaateteldooapfecaotnusrterahinisttso. gMraamgnoufssthoen s[2u8r]fapcreoptoopsoedgraanphayp.pTeawraonfcrea-mbaessecdalnosoapremeeftfheocdtivthelayt\ncmreaatctehdeda afenadtugroeohdisrteosgurltasmaroefothbetasinuerfda.ce topography. Two frame scans are eﬀectively matched and\ngood results are obtained. 3. Simultaneous Localization and Mapping Framework\n3. Simultaneous Localization and Mapping Framework\n3.1. Overview 3.1. Overview\nBased on GICP with high registration accuracy, the 3D point cloud registration between conseBcausteidveonfrGamICePs,wciotnhsheicguhtirveegiksteryatfiroanmaecscuarnadcyl,otohpe 3fDrapmoeisntiscluosueddretogicsatrlacutiloantebethtwe etreanncsofnosrmecauttiiovne fproasme.esG, rcaopnhseocputtiimveizkeedySfrLaAmMesfaonr dunlodoeprgfrroamuneds ims uinsiendgteoncvailrcounlmateentht eistrcaonnssftorurmctaetdiownipthoskee.yGpraopseh, oropatidmwiazeydpSlaLnAeManfdorlouonpdearsgcroonusntdraminitnsi.nAgneonvveirrovniemwenotf itshceopnrsotrpuocsteedd awlgitohriktehympforsaem, reowaodrwkaiys gpilvanene ainndFilgouorpea1s,cwonhsitcrhaiinstsb.aAsendoovnerfvoiuewr mofatihnepparroaplloesletdhraelagdorsi:thLmiDfAraRmoedwoomrketiesrg, ipvleanniencFoingsutrreai1n,tw, lhoiocph icsobnastsreadinotnafnodurpomsaeinopptaimrailzlealtitohnr.eaAdsss: hLoiwDAn Rbyodthoemoertaenr,gpelabnacekcgornosutrnadinitn, ltohoepflcoowncshtraaritn. tTahnedgpreoesne oarprtoimwizinadtiiocnat.eAs sthsehoinwpnutboyfththeeo3rDanpgoeibnat cckloguroduinndfoirnmtahteioﬂno,wthcehabrlat.ckThaerrgorweeinndaricroatwesinthdeiciantpeus tthoef itnhpeulatsoefr tohdeo3mDetpeor iinntfocrlomuadtioinnf,oarnmdatthioenr,etdhaerbrolawckinadrircoawtesintdheicnatoedsethcoenisntprauitnot fintfhoermlaasetiroondinopmuetteinr itnhfeorpmosaetiognra, pahndotphteimreizdatairorno.wFiinnadlilcya, ttehsethoepntiomdiezecdonpsotrsaeinint fionrfmoramtiaotnioannidnptuhet ipnotihnet pcloosuedgrmapahp oinpftoirmmizaatitoionna.ftFeirnnaolliys,etrheemoopvtaiml aizreedobptoasineeidn.foGrImCPat-iboanseadndfrathmeepaonidntfrcalomuedremgaisptriantfioonrmreastuiolnt isafttheer ninoiitsiaelrveamluoevoafl tahreeroebgtiastirnaetdio.nGbIeCtwP-ebeanskedeyfrfraammeeas,nadndfraGmICePreisguissterdataiognairnetsouoltpitsimthiezeinthiteiaklevyaflruaemoef, tahnedrethgeistorpattiimonizbeedtwkeeeynfrkaemyefrpaomsees,isanudseGd IaCsPthiseupsoedseangoadine toofogprtaimphizSeLtAheMk.eAycfcroarmdein, gantdo tthhee ochpatirmacitzeerdistkiecys ofrfatmheerpooasdewiasyu,stehde agsrothuendpopsleanneodanedoflogorpapdhetSeLctAioMn.fAracmcoeradriengintnootvhaeticvhealyraecxtetrraiscttiecds oasf tthheeropaodsewacyo,ntshteraginrot uonpdtimpliaznaetioanndcloonosptrdaienttesc,tiaonndfrtahmeecaurme uinlantoivveatievrerolyr ecxaturasectdedbyas tthhee plaosseer coodnosmtreatienrt iosprteimduizcaetdio. n constraints, and the cumulative error caused by the laser odometer is reduced.\n\nPoint Cloud Mapping (Roadway point cloud\nnoise removal)\n\n3D Point Cloud Date\n\nPlane Detection\n\nLidar Scan Odometry\n\nSegmentation and Floor Detection\nFloor Node\n\nDown Sample\nGICP Scan Registration\n\nPose Estimation Result\n\nPose Graph Optimise\n\nLoop Detection\nGICP Scan Registration\nLoop Node\n\nLidar Key Frame Odometry\nGICP Scan Registration\nKey Pose Node\n\nFFiigguurree 11.. GGeenneerraalliizzeedd IItteerraattiivvee CClloosseesstt//CCoorrrreessppoonnddiinngg PPooiinntt ((GGIICCPP))-- ssiimmuullttaanneeoouuss llooccaalliizzaattiioonn aanndd mmaappppiinngg ((SSLLAAMM)) ssyysstteemm ffrraammeewwoorrkk..\n\nSensors 2019, 19, 2915\n\n5 of 19\n\n3.2. GICP Method Description\nGICP is based on the addition of a probability model to the standard ICP minimization step, with the rest remaining unchanged. The standard Euclidean distance is used instead of the probability measure to calculate the correspondence, thus maintaining the principle advantage of GICP over other fully probabilistic techniques.\nFor a two-frame point cloud A= {ai}, B = {bi}, the transformation matrix between the two frame point clouds is T. The B point cloud is converted to the A point cloud according to the transformation matrix, and then the corresponding nearest point M = {mi} is found under the A point cloud. There are two frames of point cloud Mˆ = {mˆ i}, Bˆ = bˆi in the probability model, and the points in the point cloud M, B obey the distribution mi ∼ (mˆ i, CMi ), bi ∼ (bˆi, CBi ), and the correct transformation matrix T∗ between the two frames satisﬁes the Formula (1):\n\nmˆ i = T∗bˆi,\n\n(1)\n\nwhere CMi and CBi are covariance matrices. For a point pi in a given point cloud P, Formula (2) is used to calculate its mean and covariance matrix:\n\nxi\n\n=\n\n1 k\n\nk\n\npi, C\n\n=\n\n1 k\n\nk\n\n(pi − xi) · (pi − xi)T.\n\n(2)\n\ni=1\n\ni=1\n\nAt the same time, the Singular Value Decomposition (SVD) decomposition of the covariance\n\nmatrix C is performed to obtain the U, V matrix. To ensure that each observation point only provides\n\nconstraints along its surface normal, the singular value matrix is set to Σ = diag(ε, 1, 1), where is a\n\nﬁxed constant much less than 1, usually 0.001. The ﬁnal covariance matrix can be obtained by using\n\nthe Formula (3):\n\nCMi = UiMΣUiMT, CBi = UiBΣUiBT.\n\n(3)\n\nFor a rigid body transformation matrix T, Formula (4) is the distance between two registration\n\npoint clouds:\n\ndTi = mi − Tbi.\n\n(4)\n\nThen, Formula (5) is the Gaussian distribution of the point cloud distance:\n\ndTi ∗ ∼ (mˆ i − T ∗ bˆi, CMi + T ∗ CBi T∗T) = (0, CMi + T ∗ CBi T∗T).\n\n(5)\n\nUsing Formula (6), the transformation matrix can be obtained from the maximum\n\nlikelihood estimation:\n\nT ∝ arg max\nT\n\ni\n\np(dTi )\n\n∝arg max\nT\n\ni\n\nlog(p(dTi ))\n\n∝ arg max\nTi\n\n(dTi )T(CMi + T ∗ CBi T∗T)−1(dTi )\n\n(6)\n\n3.3. Laser Odometer\n\n3.3.1. Laser Odometer between Consecutive Frames\n\nIn the proposed system, the LiDAR pose is ﬁrst estimated by applying GICP scan matching\n\nbetween consecutive frames. For 3D LiDAR, GICP exhibits more reliability than other scan matching\n\nalgorithms (such as ICP). There is almost no new information and accumulated linear error when\n\nprocessing the matching of consecutive frames. The GICP can be used to match the real-time input\n\npoint cloud with the previous time frame to obtain the relative pose ∆Tt−1,t from t to t − 1, and the point position of the cloud at t − 1 is Tt−1. Therefore, the 3D LiDAR pose at time t can be calculated by the following Formula (7):\n\nTt = Tt−1∆Tt−1,t.\n\n(7)\n\nSensors 2019, 19, x FOR PEER REVIEW\n\n6 of 19\n\n. Tt = T ΔT t −1 t −1,t\n\n(7)\n\nSensorBsy20c1o9,n1t9i,n2u91o5usly using GICP for iteration, the pose of the 3D laser at all times can be obtain6edof. 19\n\n3.3.2. Laser Odometer between Consecutive Key Frames By continuously using GICP for iteration, the pose of the 3D laser at all times can be obtained. The key frame concept was originally used for visual SLAM, which can greatly improve\n\nc3o.m3.2p.uLtatsieornOaldeofmficeiteenrcbye.tEwsepeenciCalolnysietccuatnivenKsueyreFtrhaamtetshe algorithm can run in real time for larger\n\nenviroTnhme eknetyal fmraampes. cWonhceenpstelwecatsinogrkigeiynafrllaymuess,eidt ifsonrevceissusaarlyStLoArMed,uwcehtihche mcaantchgirnegatelyrroimr wprhoivlee rceodmucpiuntgatrieodnuanl deaﬃnctikeneycyf.raEmspeesctioalslyaviet ccaanlcuenlastuiornest.hTaht itshiesablegcoaruitshemthceasnpraurnseinkeryeafrlatmimees fworilllalergaedr teonivnicrroenamseednutanlcmeratpaisn. tWy ohfenobsseelervctaitniognkbeyetfwraemenesf,raitmisesn,eacnedssiatriystvoerreyduuncfeatvhoerambaletcihninogpteirmroizrawtihonil.e Trhedeumcianpgqruedaluitnydiasnpt okoery. fAracmcoersditnogsatovethcealkcueylatfiroanmse. TsehliescitsiobnecmauetsheotdheosfptharesveiksueyalfrSaLmAeMs wanildl ltehaed ftiorsitnfcrraemaseepdouinntc,etrhteaicnlotyudo,f iosbusseerdvaatsiotnhebketewyeferanmfrea.mThese,kaenydfirtaims evesreyleuctniofanviosrpaebrlefoirnmoepdtibmasizeadtioonn. ttTﬁhhhreesecktomefrnyaasmfpercaequmuptieaovliejinutyftdr, agitshmmepeeocsnlo,otrtu.chrdeAit,pceicsrooisuorensdeiTfdnko−g1raosttfohtthethehreeekaekkly-e−tyif1mrfaremkaepmeyo.eifTnrshatemelcelkecoetuaiyodnnf.drAamtmhceceetohprsodoedlsieneocgtofitfotohtnhetihesveipksoeudrkafoeolmyrSmLefrtAeaedmrMbbeeaaatsnwrededeteTohknn.e AthseshkoeywfnrainmFeijguudrgem2,etnhtecrreitleartiiovne ftroarntshfeorrmeaal-ttioimneppooseinotfctlhoeudco. nAticncuoorduisnkgetyo ftrhaemoedcoamnebteerobbetatwineeedn btyheucsionngsethcuetFivoermfrualma e(8s,):the pose Tk−1 of the k − 1 key frame and the pose of the k key frame are Tk.\n\nAs shown in Figure 2, the using the Formula (8):\n\nrelative\n\ntransfΔoTrkm−1, ka=tTiok−−n11Tkpo= seΔ0oRf\n\nthΔes\n1\n\nco. ntinuous\n\nkey\n\nframe\n\ncan\n\nbe\n\nobtained by (8)\n\nThe\n\nEuclidean\n\ndistance\n\nbetwe∆eTnk−c1o,kns=ecTuk−t−i1v1Tekk=ey\n\n∆R ∆s fra0mes 1is\n\n. Δs =\n\n(8) Δx2 + Δy2 + Δz2 , the rotation\n\nwbwbeehthtweewnneTeeaaehnnnneyyccEoooounnffcssltetihehdcceueeutattthinhivvrreedeeeikeskcetecoayyonnfncdfrredaiatmibimtoeieonteswnssiseiss∆en∆ΔsΔrrcs,o==∆n,arsra,ecΔracccrnoucs,dto(iatvstrnieamdtcrkeaee(ct2eΔiy∆(mR∆2ft)Rreea−)x−m1Δc1)eet,e,sadaiensnsxddca∆ettsheshedeets=ctchrariritetseeser∆rhitioxootn2hlnd+rff,eoots∆rhrhtyetoh2hnlede+Tk,khe∆teehyyzep2fnfr,oratiahTmnmehteeercoliiosptsauottthdihinoaaittnst ctlhoeudkeiys tfhraemke.y frame.\n\nFFiigguurree22..SScchheemmaatitcicddiaiaggrraammoofflalasseerrooddoommeeteterrbbeettwweeeennccoonnsseeccuuttiviveekkeeyyffrraammeess..\nAAfftteerrddeetteerrmmininininggtthheekkeeyyffrraammee,,tthheeGGIICCPPrreeggiissttrraattiioonnaallggoorriitthhmmiissuusseeddaaggaaiinnttooooppttiimmiizzeetthhee ppooiinnttcclloouuddrreeggiissttrraattiioonnwwiitthhtthhee iinniittiiaall rreellaattiivvee ttrraannssffoorrmmaattiioonnppoossee..TThheefﬁinnaallttrraannssffoorrmmeeddppoosseeiiss aapppplliieeddttootthheeccoonnssttrraaiinnttbbeettwweeeenn tthhee ppoossee aanndd tthhee ppoossee iinntthheeppoosseeggrraapphh.. TThheeppoosseennooddeeisisththeesseennssoorr ppoosseeoobbttaaiinneeddbbyytthheeaabboovveellaasseerrccoonnttiinnuuoouuss ffrraammee..\n3.4. Graph SLAM Optimization 3.4. Graph SLAM Optimization\n3.4.1. Loop Detection 3.4.1. Loop Detection\nConsecutive frames and consecutive key frames laser odometer only consider the correlation betwCeoennsaedcujatcivenetftriammeeasnadndadcjaocnesnetcukteiyvefrkaemyefsr.aHmoews elavseerr, tohdeoemrreotrergeonnelyratceodnsbiydethr ethperecvoiroruelsastitoante bwetilwl ieneenvaitdabjalcyeanctctuimmeulaantedtaodtjhaecennetxkt estyatfera, msoetsh.aHt othweecvuemr,utlhaetievreroerrrgoernweirlal toecdcubryinthtehpe rwevhiooluesSLstAatMe . wLiolnl gin-teevrimtabeslytimacactuesmwuillaltbeetuontrheelianbelxetasntdatwe,ilslonotht abtetahbelectuomcuonlasttirvuectegrrloobr awllyillcoonccsuisrteinnt tthraejewcthoorilees SaLnAdMm.aLposn, ags-tsehrmowenstiinmFaitgeusrwe 3il.l be unreliable and will not be able to construct globally consistent trajectories and maps, as shown in Figure 3.\n\nSensors 2019, 19, 2915 SSeennssoorrss22001199, ,1199, ,xxFFOORRPPEEEERRRREEVVIIEEWW\n\n7 of 19 77ooff1199\n\nFFFiiiggguuurrreee333...SSSccchhheeemmmaaatttiiciccdddiiaiaagggrrraaammmooofffttthhheeellolooooopppoooppptttiimimmiizizzaaatttiioioonnnppprrroooccceeessssss...\nhskFhshiiehisgoysotuwtowofrWWrrWnerianichcm4hiahina:eenleelnnFkn,Fkiepaiegpygnpeyueurdefrrffrrreofratefoarhom4rmm4er:m:menien,i,isnagnaegnglnledodlcolottopthohtoehpdepneendtcsdeseeaceetlnttleeieedoccccttinttditio,tohaﬁnhnter,ee,scltfcafoiacirnornosdsptmdtiidfcpdcroaaaoamtrmmteeepelptloa.hoaoreTorepephcufetftrhrrhafreaoememlnclecotue.uw.rrTrerTirahenhelne-gnettfticfomororleleneloaoadklwl-wie-ttitiyiinimonmgfngreeasccmokaoknereneedydywimitftiifrtioerhoatnamn,mtsahseaseearsrhwewheimisomitttwhoehertnt,it,cthaihaanesles\n111... TTThhheeeiininndddeexexxoooffftththheeeccucuurrrrrerenenntttkkkeeyeyyffrfraraammmeeeiisissllalaarrgrggeeerrrtththhaaannntththheeeiininndddeeexxxoooffftththheeehhhiisissttotoorrriiciccaaalllkkkeeeyyyffrfrraaammmeee;;; 222... TTThhheeedddiififﬀfeeerrereennncceceebbbeeettwtwweeeeeennntththheeettrtrraaajejjeecccttotoorryryydddiisissttataannnccceeesssooofffttthhheeecccuuurrrrrreeennntttkkkeeeyyyfffrrraaammmeeeaaannndddttthhheeehhhiiissstttooorrriiicccaaalllkkkeeeyyy\nffrfraramammeeeiisissgggrrereaeatatetererrtththhaanannaaassesetetttththhrrereesshshhooolldldd;;; 333... TTThhheeerrerelelalatatitivivveeettrtraraannnsslslalaattitioioonnndddiisiststataannncceceebbbeeettwtwweeeeeennntththheeecccuuurrrrrreeennntttkkkeeeyyyfffrrraaammmeeeaaannndddttthhheeehhhiiissstttooorrriiicccaaalllkkkeeeyyyfffrrraaammmeeeiiisss\nlleelsessssstththhaanannaaassesetetttththhrrereseshshhooolldldd...\n\nFFFiiiggguuurrreee444...SSSccchhheeemmmaaatttiiciccdddiiaiaagggrrraaammmooofffllolooooopppdddeeettteeeccctttiioioonnn...\nFFFiiinnnaaalllllyyy,,,aaaccccccooorrrdddiiinnngggtttooottthhheeeooobbbtttaaaiiinnneeedddcccaaannndddiiidddaaattteee lllooooooppp fffrrraaammmeee aaannnddd ttthhheee cccuuurrrrrreeennnttt kkkeeeyyyfffrrraaammmeee fffooorrr GGGIIICCCPPP rrreeegggiiissstttrrraaatttiiiooonnn,,,ttthhheeehhhiiiggghhheeessstttGGGIIICCCPPPrrreeegggiisisstttrrraaattitioioonnnssscccooorrreeeiisissssseeelleleeccctteteedddaaassstththheeeﬁffininnaaalllllolooooopppfffrrraaammmeee...TTThhheeekkkeeeyyyfffrrraaammmeeeaaannnddd ttthhheeellloooooopppfffrrraaammmeeeaaarrreeeaaaddddddeeedddaaasssnnnooodddeeessstttooottthhheeegggrrraaappphhhSSSLLLAAAMMMoooppptttiiimmmiiizzzaaatttiiiooonnn,,,aaannndddttthhheeeeeedddgggeeecccooonnnssstttrrraaaiiinnntttiiisssttthhheee rrreeelllaaatttiiivvveeepppooossseeeooobbbtttaaaiiinnneeedddbbbyyyrrreeegggiisisstttrrraaatttiioioonnn... 333..4.44..2.22...RRRoooaaadddwwwaaayyysssPPPlllaaannneeeDDDeeettteeeccctttiiiooonnn\nThe underground mine environment is usually dominated by roadways of equal distance, as showTTnhheienuuFnnidgdeuerrrgegrro5ou.unnWddhmmeniinnteeheeennrvovibirrooontnmimseeonnptteiirssautuisnsuugaalilnlyydddioﬀomemrieinnnaatttesedpdabbtyiyarlrooraeadgdwiwoanaysys,stohofef eetququunaanlledldipisstltaaannncecee,c,aaanss spshrhooowvwindneiinandFFdiigigtuiuorrenea55l..cWoWnhhsetenrnaittnhhtees rfrooobrbomottoiistsiooonppeeersraattitiminngagtiiinonnddoiiffffekerereyenntftrsaspmpaaettisia.allTrrheegegiisooannmss,,etthpheleatntuuanrnnnneeolldppelslaancneaenccabanen popbrrosovevriidvdeeedaaduddndidittieioornndaaillﬀcceoornensnstttrrapaioinnsttsesfnfooorrdmemsoo.ttiTioohnneeepsstutiimrmpaoatstiioeonnofoofefskkteaeybylfifrsrahamimnegess.p.TlTahhneeasrsaacmmoneesptprllaaaninnaatrsrninsoodtdoeespscrcoaanvnidbbeee oaodbbsdseiertrivoveneddaluunnnodddeeersr dtdoiiffofeperrteeinmnttipzpeoostsheeennpoooddseeess.g. rTTahpheehp(psuuirmrppioloasseretoooffleoesostptaabdblleiistshehciintnigognpp)llwaannhaairlreccrooendnsusttrcraianiingnttstshieisscttuoomppurroloavvtiiidvdeee aeeaerdrrdrrdroodoirritrtibbioboyynynauuaulsslsniininnonoggdgdemmemssoototorroreeeooccpcpootontninimssmsttitrrizraazaeiienintnthtthteiieninnpfpffooooorsrsrmmemegaagarttratiiaoiopopnnhnhtt(too(sosiiiimmimmmippilplararrroorotvvtoveoeeltlotohthohoeepepaadacdcceccecututeurercracatacticiocyyoynnoo)o)ffwfwppphoohoissilseleeeeerereessestdtditimuimumcacaiainttnitigoiogontntnh..h.eeccuummuullaattiivvee\n\nSensors 2019, 19, 2915 Sensors 2019, 19, x FOR PEER REVIEW\n\n8 of 19 8 of 19\n\nFFiigguurree55.. SScchheemmaattiiccddiiaaggrraammooffrrooaaddwwaayyppllaanneeccoonnssttrraaiinntt..\n\nTThheekkeyeytotoesteasbtalibshliisnhginthgetphleanpelacnoensctoranisnttriasintot aiscctuoraatceclyureaxtterlayctetxhteraroctadthweayropaldanwea, yanpdlachnoe,osaenad\n\nrcohbouossteesatimroabtuiosnt emsteitmhoadtiownitmh leetshsoidterwatiitohnlseasnsditsetrraotniognrsesaisntdanscterotonggrroesssisetrarnorce(ntooisgerdoastsae)—rroRran(ndooimse\n\nSdaamtap)—le RCaonndsoenmsuSsa(mRpAlNe SCAoCns)e[n29su].sT(hReAcNomSAmCo)nl[y29u].seTdhpelacnoememquonatlyionusisedtheplnaonremeaqluoafttiohne pislanthee: anxo+rmbayl+ofctzh=e pdl,awneh:eraexa+2 b+y b+2c+z =c2d =, w1h, der>e 0a, 2(a+,bb2, c+)ci2s =th1e, dpl>an0e, n(oar,mb,acl)viesctthoer,palnadned nisotrhmeadlivsteacntocer,\n\nfaronmd tdheisotrhigeinditsotatnhceepflraonme.tThheeosreigfoinutroptahreampleatneers. Tcahnesdeeftoeurmr pinaeraampeltaenres.can determine a plane.\nSSiinnccee tthhee rrooaaddwwaayyfﬂloooorrisisrurugggegded, i,t iits insont optospsoibssleibtloe ctooncsotrnuscttruacptoasepgorsaepghrwapithh wa iftihxead ﬁplxaende palsaanenoads eafnoor doeptfiomriozapttiiomni.zIantioornd.erIntooerndseurrteothenatsuthree cthoantsttrhuectceodnpstorsuectgerdapphosceongfroarpmhsctoontfhoermacstutaol tshietuaacttiuonal, stihtueartiooand,wthaeyropaladnweayexptrlaancteioenxtriasctpiroonpiosspedropboasseedd boansetdhoenstuhbe-msuabp-.mAapcc. oArdccinogrditnogtthoe tIchnoeoncrsdotrneursctttroeudecntleosducarloel cptaholeipneotﬃicncltoiecunldocuymdoafmpt,hatpeh,retohateudntwunaneynl epplllaapnnlaeenepexaptrraaamrcatemiotenert,setrhsπemπcm=ur[=nrea ,n[nntbap,,nnocbs,,ditn]ico,andr]iesaertaexkterexantcrtaaescdtt.ehIden. oorridgienr, taondentshuerpeotihnet eclfofiucdieniscsyeaorfcthheedrionatdhwe asyubp-mlanape ewxittrhaicntitohne,rtahdeiucsurorfetnhteplaosseitriorandiasr’tsakmeenasausrtehde dcodoirisnsitgtvaaiennnrc,cteeae.dn. AdAtoccthcctohoererpddsoiienninnggsttoctoorlotlthuohedcea3il3sDcDsoLeoLairDridDcAhiAneRadRtpeiponssoyteshseteeppsmttuabbat-ytmtthuhaissipisnmwmgoioFtmhmoirenemnntthut,,elttahhrseae(d9ttuiu)unnasnnnodeefllt(pph1l0leaa)nln:aeeseppraarrraaadmmaeret'tseermrsseππamsmuaraerrede converted to the sensor local coordinate system by using Formula (9) and Formula (10):\n\n[na[,nna' b, ,nnb' ,cn]Tc' ]T==RRtt·⋅ [[nnaa,,nnbb,,nnc ]cT]T, ,\n\n((99))\n\nd =d ' d= −d −tttt· ⋅[[nnaa' ,, nnb'b,,nnc' c]T]T, ,\n\n((1100))\n\nw[awRnhth,deetrtrTe][ehRiπsetπm,tethm't=r]er=os[i[rensnnaba't,she,nontebwbr' ,,spneneecocn' ,,nssddeot' rh]]aietpsitosp.tshoteheseaectoncooot rdo.deridnaianntdaettoehfeotfhtuethnrenoaerdlopwaldaawyneapynlaopndleaeinn[e3t0hi]neistshecnealscsouernlacsotoeordrdcaoisnoFardoterinmsyautseltaesm(y1,s1ta)en:md, The error between the pose node and the tunnel plane node [30] is calculated as Formula (11):\n\nei,mei=,m =q(qπ(πmm' )) −− qq((ππt )t)\n\n((1111))\n\nwwhheerreeq(qπ(π) )== [aarrccttaann((nnnnbaba)), a, arcrtcatna(n|nnc|nn|c)|, d, d] .. 3.4.3. Graph Optimization Construction 3.4.3T. GheragprhapOhpotpimtiimzaiztiaotnioCnownistthruscetniosonr poses and spatial points is called Bundle Adjustment (BA), whichTchaengerﬀapechtiovpetlyimsiozlavteiolanrwgei-tshcaselenpsoorsiptioosneisnagnadndspmataiaplppinoginptsroisblceamllesd. HBouwndelveeAr, dasjutsimtmeegnote(sBAby), twhehticrahjeccatnoreyffoefcmtivinelinygsoeqlvueiplamrgene-tswcailel bpeocsoimtioenlionnggearnadnmd alopnpginerg, apnrdobtlheemms.aHp oscwaleevweri,llacsotnimtineugeoteos gbryo,wt.hTehteraBjAectmoreythodf mwilnlirnegduecqeutihpemceonmtpwutilaltiboencaolmeﬃe clioengcye.rTahnedpolosneggerra,pahnodpttihmeizmataiopnspcaroleviwdeisll aconnetwiniudeeatoto gsorolvwe.thTihseprBoAblemme,tahnoddownliyllrergedarudcseththeme caosmthpeuctoantisotnrailnet fofficpieonsceye.stTimheatpioons,eangdranpoh lopntgiemriozpatiomnizpersotvhiedepsoaseneeswtimidaetaiotnoosfoflevaetuthreispporionbtsle. m, and only regards them as the constraint of poseBeassteimd aotnionth, eanpdosneogloranpgherooppttiimmiizzaetsiotnhethpeoosreye, sbtiamseadtioonn otfhfeeacatulcruelpatoeidntsk.ey point pose, plane constBraaisnetdanodn ltohoeppdoesteecgtriaopnhcoonpsttimraiznatsti,otnhethceoonrsytr, ubcatsioedn oofnththeegcralpchulSaLteAdMkeoyf tphoeinutnpdoesreg,ropulannde mcoinisntrgaeinntvairnodnmloeonpt disecteocntsiotrnuctoends,tarasisnhtso,wthneincoFnigsturruect6i.on of the graph SLAM of the underground mining environment is constructed, as shown in Figure 6.\n\nSensoSrsen2s0o1r9s,21091,92,91195, x FOR PEER REVIEW Sensors 2019, 19, x FOR PEER REVIEW\n\n99 ooff 1199 9 of 19\n\nFigure 6. Schematic diagram of the pose structure SLAM, {p1 , p2 ,... pn+ 2 } is the key frame pose\n\nFnFioiggduuerre,e6{6.π.S0Sc,hcπhe1me,m..a.πatitcnic}didaiisgartgharmeameoxftortfahcetthepedopsreoassedtrwustcartyuucgrteruoSrueLnASdLMpA,lMapn1,e, {pc2op,e1.,f.fp.icp2in,e.+n..2tpnnios+ 2dt}hee. iksetyhefrakmeye fproasmeenopdoese,\n\n{nπo0,dπe1, , {. .π. π0 ,nπ}1i,s..t.hπen }extirsatchteedexrotraadcwteadyrgoaroduwnadypglraonuencdoepﬃlacnieenctonefofdicei.ent node.\n\nThe pose graph optimization problem can be effectively solved by standard optimization\n\nstttctmincaomincaohhoouonnoonpnepeeerctestetrrcdcrdttehehotocrtrieeieghghmeomeelpTsssarrvTrsorsorpssothosaaiapierphidadazozaaemebtttoroerstaesanedhdsoyphyn,n,dtipdtdwewzviioepdsdsoooiaetGstuunoaiaiipinnonotennnnycsygiclaodoagehlgghlssusiciicntnstrbnbehoehsoeeppaoagarrgsldndenergspdlsirala-abagsgasNrphrcertotntorntaepyeyoshohrraepopepsesuhnuseera[w[.p.tthe3y3cscndidiAAtGGm2temto2tndioeGeomc]c]cpaatoptitnitucceeecezcuzthuhditoodniactaczaaiosesmeeirtmrntengatsvtsrisdidiirto-eo-ioe,cocbiLbiaNizNisnznoolnonnneleaekoa,nec,regrgGtvoeutruarwwilclcipeyeopeitritnsosoobobonsatrstneenrrfonooprbprpbddrrtrtaannboeohohhepaeeprrtdltsnrmesnerycyreoooOogppoeododmararessobsopnbGMiGibbsoonnnLlnLatslcooeeleielgtogandeadevmvmvnrvnmrvnddviuieeeeeqennippe.eoczbrrputtglgnlnotomcahaahaeaelabasblstlasesendnreieeeneeeneodGﬀoG,potprrrrenetadgedgnonobrrccr(nnc(aeaose,osGeooLMtMdoepsepsdldinnM2ovdhehgegeasatasaOoefehseserrn)rrnflps,)Oa,qcOeaqcyed[d.upup3uaculGdpTlpsto1ntohthaeaeooiteiti2]oodvrvdirvdiltoOommddvpdapeeegegptpleoclttheiegiykoktdttzznns(esi(tiiopeeamLahmLoioobyrsytrctctMinMdidioyiiaisaitmcofofzzel,hennraros)v)nanasisaiamtnrtezt(t[a([bmbmriiG3dG3idaan.oeaoese11tensen2dn2Iniro]]pstosbccOdOodanpopooanhyaorao))lnrdlnnrlad.dydp.pglgoosdsssiowlooolTboTbntbttaaiarmaprmrrrnoleolnanuuleiaeintcgrttyeeeeimcmrchdeehdmdtystttesameameeoe.o.nndsidpr[rrddzao3o,.g,.dylta,tr,a2dldlheIhIytooai]tnatseieioinooons.nstcesshhpto,pn,paddAeaadtnaaicgiimsdsdctetmtnotnrcbhehetaaaneddioieetiteselstltzseercrrheutdaccetetGtGodrooithsthtaiauoniniidne22edoedioocnldnlOgsOiinyytnnyyr,,r\n\n33..55.. PPooiinntt CClloouudd MMaapp CCoonnssttrruuccttiioonn\n\n3.5. PTTohhienetmmCalapopuppdiniMnggafrpferCqeuqoneunsetcnryucycistiiolsonwloewrethr atnhatnhethlaeselrasoedroomdeotmereftreerqufreenqcuye. nAccyc.oArdcicnogrdtointghetoabthoveealbaosevre cloansesreTccuhotenivsmeeckaueptypivfinreagmkfeeryeoqfdruoaemmnceeyteorid,swolmohweenteerar,tfhrwaamhneetnhpeoailnfartsacemlroeuoddpooismindetettceelrcotfuerddeqaiusseadnkecteyey.cAftreacdmcoaers,dtaihnekgeutynodftreharegmraoeb,uonthvdee eulnanvsdeirreorcngomrnoesunenctdumteiavnpevnikreoeeyndmsfrteaonmbt eemuoapdpdonamteeeedtde.srT,thowebhmeenauinpadmfaretaetmhdo.edTphioseitnomtcacoilnnovumedretitsthhodederteiesacltt-oetidmcoaenspvaoeirkntettychleofruraedmaole-ft,itmthheee kpueonyidnfertracgmlrooeuudinndtoofettnhhveeirwkoeonyrmlfdreancmtoomeriadnpitnonaettheeedsyswstotoermlbdetcuhoproodrudagtienhdac.toeToshryedsitmneamatienthtmrraoenutshgfoohdrcmiosaottroidoincno,anatvenedtrrtaconthmsefoprrremeaslas-ttitiohmnee, pikcactipatisoshoshoknnoooieeoooeddnibrnrbybdttWdckckttaWtWicfoaoainirclnnhimikmkoahalhnnaeoeeeumteepepdtneuyyndneddrsredsbteeffttthcbbrrisyhshchonaaeysysaeernfummnoktttuushthhubeke+hkisbseeenegiiee++iinngpnh1pnwo11ogkgoobtfttbotehrihihtthtffyntranhaehreraeelmtitaeadeGifnownmcwrmceGGecIalcleoCodooteoemiIdIrsuurCrCPoieibllesbdssdrdePdPyfedyssoitstlencfecftihehrttonohllotrhcrerecorruaeootooeccoetrccrctuctutnedcddeoteohogsguotddinineoi.ahnhnornttrsaeaTarwiaiudttnsnsdttthhh[oiheoeu3utitneeeushnsher3ooa..lsoeaope]udkutT.TtccekokkesseththceseyertrkkteoyereyeyrefaeoeeaprTpffyfnyrranrrsoskoWdasaamft+sftssfmrrirrmmfoe1enuauaoeremaem,ecrc=mTTmt,tt,tokeuhkeueWtWt+a+pThh1a1errsototee=ktWe=iyirpipomoTerT[sr[Ttnt3ke3keltnWiWiiakLe.m3lm3lzTT.+aatm]kF]akFiLLt+.t1i+.ivt1ii1ziziinvvneotoaahoaeoefatntpilfriflptp.oloolohyttyonoAnsuh,he,ses..tegecteehkAAcThheo+kekkLccTT+rcpc+kc+kpdLL+o+1o1o11o11oiorrnibkidrnbdkbkngeedeiteeiettynnittywtycnwcowgglflaoeffroteertrtteauhoouaeeaenmdenmnmdtttthphercttheceahehooeoieniensioeponpneskrkkrotofdetTdthoeesshyhiykryWieeenenemfaffwraowrTwrTtaataeakfkoWemWtoomsmitrsorhrleoleooledonesddfssfff,\n\nttchhoeeokrdk++in1aktkeeesyycffarranammbeeeaoarbreteacciononenvdveebrrtyteedtdhteotoctohthoeercdcoioonoradrtdeinitnaratetneQsfkoQ+rkm1+1ianintitohthnee.wFwionoralrdlldyc,cotohoredrdpinionaiatnettescysloystusetdmemcoboybryTdkWiT+nkW+1a1,t,eaasnndodf\n\ntthheeppokoi+inn1ttckclelooyuudfdrammmaaeppairisseuucppodndavateteerddtebdbyyttothhteehoeoccttroreoeeredsstitrnruauctcteuturQree.k.+A1AissnsshthooewwwnnoirinnldFFiciggouuorrede7i7n..ate system by\n\nTW k +1\n\n,\n\nand\n\nthe point cloud map is updated by the octree structure. As shown in Figure 7.\n\nFFiigguurree77..PPooiinnttcclloouuddmmaappccoonnssttrruuccttiioonnddiiaaggrraamm..\nFigure 7. Point cloud map construction diagram.\nIInntthheepprroocceessssooffccoonnssttrruuccttiinnggtthheeppooiinnttcclloouuddmmaapp,,wwhheenntthheekkeeyyffrraammeeiiss aaddddeedd ttoo the sub-maapp,,\ntthhee ppoIoniinntthtecclplooruuoddcennsosoioissfeecaoatnt tsththreeupcptooisnsegeitishserrepemmooionvvteecdldoauacdcccomorrdadpiinn,gwgthtooentthhteheecchkhaearyraafcrctateemrriisesttiiicscsasdoodffetthdheetouutnhndedeesrruggbrro-omuunandpd, rrtohoaaeddwpwoaaiynytttoocleoennussduurnreeotithshaeattatththetehcceoonpnsosttrsrueuccitsteedrdemmmaoapvpessdaattiasiscﬁfceioesrsdtthihneegaactctotuuatahllsesitictuhuaaatrtioaiocnnteaarsissmtmiucuscchohfaastshppeoosusnsibidbleeler..gAArottutthnhede ssraaommadeewtitmiamyee,t,oitietisnissaulasrloseopthopasostsitbshilbeeltceootnroesmrtreuomvcoteevmdeommvaionpvgisnaogtbisjoefbcietjsescstthusecshaucactshupaaelsdspeitseutdraeitasiontrnsi,aasnsosm,thsuaoct ththhaaestrptehgoeisssrtierbaglteiiso.tAnratatintohdne pasoansmditeipootnismiintigeo,rnietisniusgltarselsasourelptmsosaosrrieebmlreeoltiroaebrrleem.liAaobvsleesh.mAoowsvnsihniongwFonibgjiuencretFsi8gs.uuTrchehe8a.sspTpehceeidﬁsepcsestctreiiafpincsssf,otesroprsethmfoaortvrtiehnmegornevogiinissgetrfnarotoiimosne tfahrneodmropatohdsewitriaooyandpinwogianrytescpuloliutnsdtacarleroemuadosrafeorerlleoalwisafbso:llel.oAwss:shown in Figure 8. The specific steps for removing noise fromSStttehepep1r1o::aTTdhhweealliyinnpeeoccioonnntnnceleoccuttiidnnggarttheheeasccufuorrlrrleeonnwttsttr:raacckk ppooiinntt wwiitthhtthheepprreevviioouussttrraacckkppooiinnttiissttaakkeennaasstthhee nnoorrmmaSaltlevvpeec1ct:otoTr,rh,aeanndlidntehtheceocanclancleucculatlitanitoginotnhmemectehutorhrdoednistiastrsaafscokflolopllwooiswn:tst:wwtwoithtoratthrcaekcppkropeinvotiiosnutAss(xtAr1a(,xcy1k,1y,1pz,o1z1)i)n,,BtBi((sxx2t2a, yk22e,nzz22a))saatrhreee\nnormal vector, and the calculation method is as follows: two track points A(x1, y1, z1) , B( x2 , y2 , z2 ) are\n\nSensors 2019, 19, x FOR PEER REVIEW\n\n10 of 19\n\nknown. Vector\nSensors 2019, 19, 2915\n\na\n\n=\n\n⎯⎯→\nOA\n\n=\n\n( x1 ,\n\ny1 ,\n\nz1 )\n\n ⎯⎯→\n, b = OB = (x2 , y2 , z2 )\n\n.\n\nThe\n\nnormal\n\nvector is\n10 of 19\n\n⎯⎯→ ⎯⎯→ ⎯⎯→\nAB = OB − OA = ( x2 − x1 , y2 − y1 , z2 − z1 ) . Moreover, the normal plane equation at the track point is\n\nkcnaolcwunla.teVdecatcocro→rad=ingO→tAo\n\n=the(xn1o,rym1,azl1v),e→cbto=r\n\nbO→yBu=sin(gx2F,oyr2m, zu2)la.\n\nT(1h2e),naosrsmhaolwvnecintoFriigsuAr→eB\n\n8=a.\n\n→\nOB\n\n−\n\n→\nOA\n\n=\n\n(x2 − x1, y2 − y1, z2 − z1). More(oxv2 e−rx,1t)h(xe−nxo2 )r+m(ayl2 p− lya1 )n(ey −eqyu2 )a+ti(oz2n−azt1 )t(hz e− tzr2 a) c=k0p. oint is calculated accord(1in2g)\n\nto theHnoowrmeavlevr,eccotonrsibdyeruisnigngthFeorrombuulsatn(e1s2s),aansdshtroawjenctoinryFiogfuthree8aalg. orithm, when the following points\n\noccur in the trajector(yx2p−oixn1t), (nxo−isxe2r)e+mo(vy2al−pyr1o)c(eyss−inyg2)is+n(ozt2p−erzf1o)r(mz −edz2h)e=re:0.\n\n(12)\n\n1. Calculate the curvature of the point, and exclude the calculation as a normal vector in the case Hoof wlaergveerf,lcuocntusiadtieorninsginthceurrovbautusrtne;ess and trajectory of the algorithm, when the following points\no2c.curCinaltchuelatrtainjegcttohreydpiostiannt,cneobiesetwreemenovthael ptrraocckespsoiningtisanndottpheerpforremvieodusheproei:nt. When the distance is\n\n1. Cgarelcautelarttehtahne acucrevrtaatiunrethorfetshheopldo,intht,eapnodinetxicsluedxecltuhdeecdalfcruolmatitohne ansoarmnaolrmveacltvoercctaolrcuinlatthioenc;ase of\n\n3. lCaraglceuﬂlautcetuthaetioansglien bcuetrwvaeteunreth; e line connecting the track point and the previous point and the\n\n2. CXaalcxuisl,aatinndgctohme pdaisrteatnhceerbeelatwtioenesnhtihpebtertawckeepnotihnet aanngdlethaendprtehveihoueasdpinoignat.nWglehoenf tthheepdoiisntta.nWcehiesn gitreisatgerretahtaerntahacenratacinerttharinesthhorleds,htohled,ptohientpiosinextcilsuedxecdlufdroemd ftrhoemntohremnaol rvmecatlovreccatlocruclatlicounla; tion.\n\n3. CSatelcpu2la:tBeatsheedanonglethbeetgwiveeenn sthecetilionne icnotnenrceecptitninggththeetrtahcikckpnoeinsst δan, dthteheppoirnevt icoluous dpobiannt danwditthhetXhe\n\nbandaiswxgiisdr,etahantedδr2ctohomannpbaaortcehetrhstiaedirenesltaohtfrieothsnhesohnliodpr,bmtheatewl ppeoleainnntethiises\n\nangle and the heading angle of the point. When it eoxbctlauindeedd.fTrohmat tish,ecnalocrumlaatlevtehcetoEruccalildcuealantidoins.tance\n\nfromSttehpe 2p:oBinatsiendtohne pthoeingtivcleonudsetcotiothneinoterrmceapl tpinlagnteh, ewthhiicchknsaetsissfδie, sthtehepFooinrtmculolaud(13b)a,nads swhiothwtnhien\n\nbFaingduwreid8btha2δndonFibgoutrhes8icd.es of the normal plane is obtained. That is, calculate the Euclidean distance\n\nfrom the point Figure 8b,c.\n\nin\n\nthe\n\npoint\n\ncloud\n\nto\n\nthe\n\nnormal\n\npdila≤nδδ2e,.\n\nwhich\n\nsatisﬁes\n\nthe\n\nFormula\n\n(13),\n\nas\n\nshown in (13)\n\nStep\n\n3:\n\nThe\n\npoint\n\ncloud\n\nis\n\nconsidered\n\nto\n\nbde i\n\n≤ on\n\n2th. e\n\nsame\n\nplane,\n\nand\n\nthe\n\npoints\n\nthat\n\ndo\n\nnot\n\nfa(l1l 3o)n\n\nthe pSltaenpe3a: rTehperopjoeicntet dcl,otuhderiesbcyoonbsitdaienriendgtao dbiescornetteheposainmt eseptloanf eth, eanndortmheapl poilnantse.thAatt tdhoe nsaomt fealtlimone, thine porladneer atroe epnrsoujercetetdh,ethneoriesbeyroebmtaoivnainl gefafedcits,carectceorpdoiinngt steot othf ethme ninoermroaaldpwlaanye.dAetsitghne ssatamnedtaimrde,,tihne orrodaedrwtoaeynsstururecttuhreeniosisdeivriedmeodvianltoeﬀaecrto,oafc,caorfdloionrgatondthtewmoiwnealrlosa, dawndaythdeepsioginntstcalnoduadrdo,f tthheerrooaaddwwaayy sstreuctcitounreisisddiviivdieddedinitnotofoaurropoafr, tas ﬂSo1o, Sr2a, Sn3d, St4w, oaswshalolws, nanind Fthigeuproei8ndt calnodudFiogfutrhee8reo. adway section is\n\ndivideSdteipnt4o: fAouftrerpaorbttsaSin1i,nSg2,tSh3e, Sd4i,satrsibshuotiwonn oinf tFhigeurroea8ddw,ea.y segment, the RANSAC method is first usedStteopfi4t: tAheftecruorvbeta, itnhienng the dinisnterribpuotionnt iosfftihtteerdoabdywthaey lseeagsmt seqnut,atrheesRmAeNthSoAdC, amndethfiondalilsyﬁtrhset uosuetder\n\ntopoﬁitntthoefceuarcvhe,ptahretnisthreminonveerdptooinotbitsaﬁinttethdebnyotihse lreeamstosvqaularroeasdmweatyhosdec, tainodn ﬁpnoianlltyctlhoeudo,uatesrsphoiwntnoifn\n\neFaicghuprear8tfi.s removed to obtain the noise removal roadway section point cloud, as shown in Figure 8f.\n\nFFigiguurere88. .RRooaaddwwaayyppooininttcclolouuddnnooisiseerreemmoovvaallﬂfolowwddiaiaggraramm. .\nIn the process of constructing the point cloud map, the above four steps are repeated for each track point, and the roadway point cloud noise is automatically removed.\n\nSensors 2019, 19, x FOR PEER REVIEW\n\n11 of 19\n\nIn the process of constructing the point cloud map, the above four steps are repeated for each\n\nSensors 2019, 19, 2915\n\n11 of 19\n\ntrack point, and the roadway point cloud noise is automatically removed.\n\n44.. EExxppeerriimmeenntt\n\n44..11.. Introduction to the Experimennttaall PPllaattffoorrmm\nTThheeuunnmmananendedopoepraetriaotnioenxpeexrpimereinmt eanntd adnedveldoepvmeleonptmpleantftorpmlaotffotrhme socrfaptheer isscmraapdeeraciscormdaindge taoccthoredrienagl tvoethhieclreeaslizveeh1:i5cl.eIstiszfeu1n:5c.tiIotsnfuisnbcatisoincailslybatshiecaslalymteheasatmhaetaosftthhaet orefathl ecarre.alItcacor.nIsticsotsnsoifstas bouf cakbeut,ckaebt,oaombo,oamc,oancnoencntiencgtirnogdr,oadr,oacrkoecrkaermar,ma,raortaortyarcyyclyinlidnedre, ra,nadnda aliflitfitninggcycylilninddeer.r.IIttrreeaalliizzeess tthhee ssiigghhtt--sseeeeiinngg reemmoottee controoll of the scraper.. It can monitor the operating conditions of the scraper, iinncclluuddiinnggiinnffoorrmmaattiioonnssuucchhaass pprreessssuurree,,ssttrrookkee,,ssppeeeedd,,aannddaanngglleeooff rroottaattiioonnttoo mmeeeett tthhee experriimmeennttaall rreeqquuiirreemmeennttssoof funumnmanannendedshsohvoevl leolaldoiandgi.nTgh. eTphleatpfolarmtfovrmehivcleehsiuclpepsourptsptohretslintheecolinnterocl olongtrico,lwlohgicihc, mwahiinchly minacilnuldyeisntchluedleinsethcoenltirnoel cmoonvtreoml emnot,vtehmeelnint,etchoenltirnoel sctoenetrrionlg satneedritnhge alinnde cthoentlrionlethcoronttrloel. Itthprortotvlei.deIts parnovexidpeesriamnenextapleprilmatefonrtaml pfolarttfhoreminfdoerptehnedeinndteoppeenrdaetinotnoopfetrhateiosncraopf etrh,eanscdraspueprp, oarntds tshueplpinoertcsotnhterolilnwe acloknintrgolawndallkininegcaondtrolilnleoacdonintrgoal nlodaudninlogaadnidnguonflotahdeisncgraopfetrhtehsrcoruagpherththersooufgtwh athree dsoevftewloapremdeenvteplolaptfmoremnt, palsasthfoorwmn, ains sFhigouwrne 9in. Figure 9.\n\nFFiigguurree 99.. UUnnddeerrggrroouunndd mmiinnee ttrraacckklleessss eeqquuiippmmeenntt eexxppeerriimmeennttaall ppllaattffoorrmm..\nTThhiisseexxppeerriimmeennttoonnlylyuusseessVVeleoldodynyen1e616asaas asesnesnosro. rT.hTehexepxepreimriemnetanltascl esnceninecilnucdluesdefosufor uscresnceesnoefs sotfrustcrtuucrteudrecdorcroidrroidr oarnadnmd imneinreoraodawdawya, ya,sasshsohwonwinninFiFgiugruere101.0F. Figiguurere1100aaisisaassttrruuccttuurreeddooﬃfficceetthhaatt hhaass nnoott bbeeeenn rreennoovvaatteedd. Figure 10b for the oﬃffice corridor. Figure 10c shows the roadway with ring. FFiigguurree 1100dd sshhoowwss tthhee sscceennee ooff tthhee mmiinnee main rooaaddwwaayy.. EExxppeerriimmeenntt aanndd aannaallyyzzee tthe four localizatioonn aanndd mmaappppiinngg mmeetthhooddss ooff LLOOAAMM [[1199]],, LLiigghhttwweeiigghhtt aanndd GGrroouunndd--OOppttiimmiizzeedd LLiiddaarr OOddoommeettrryy aanndd MMaappppiinngg((LLEEGGOO--LLOOAAMM))[[2200]],,BBeerrkkeelleeyyLLooccaalliizzaattiioonnAAnndd MMaappppiinngg((BBLLAAMM))[[3344]]aanndd GGIICCPP--SSLLAAMM ffoorr ffoouurrsscceennaarrioioss..TThheeggooaal loof fthteheLOLOAAMMis itsotboubiludiladreaarle-tailm-teimlaeselarsoedroomdeotmereutesrinugsaintgwao-tawxois-aLxiDisALRiDthAaRt mthoavtems ionvtehsreine dthimreeensdioimnse.nTsihoends.isTtohretiodnisetoﬀretciot ncaeufsfeedctbcyatuhseedmobvyemtheenmt oofvtehme eLniDt AofRtihseelLimiDinAaRtedis. Belaismedinaotnedth. eBacsoerde oidnetahoefcsoerpeairdaetainogf lsoecpaalirzaatitniognloacnadlizmataipopnianngd, omnaepips itnogp, oernfeorismtohipgehr-fforremquheingchyofrdeoqmuentecrys boudtolmowet-eprrsebciustiolnowm-optrieocniseisotnimmatoitoinon(loecstailmization)(,laoncdaltizhaetoiothne),raisntdotpheerfootrhmermisatochpinergfoanrmd rmegaitsctheirnpgoaintdcrloeguidstienrfoprominattciolonuadt iannfoorrmdeartioofnmaat ganitourderloowf merafgrenqituuednecylo(wmearpfpreinqgueanncdyc(omrraepcptiinngg oadnodmcoerterersc)t.inAg hoidgohm-perteecriss)i.oAn hanigdh-rpearel-ctismioenlasnedr roedaol-mtimeterlaisseorbotadionmedetbeyr icsoombbtainininegd tbhyectowmobpinairntsg; Tthhee tawuothpoarrotsf;LTEhGeOa-uLtOhoArMofpLrEoGpoOs-eLdOaAlMighptwroepiogshetdanadlighrotwunedig-hotpatinmdizgerdouLniDd-AopRtiomdiozmedetLeriDaAndR modaopmpientgermanedthmodapfoprinegstmimeathtiondgftohreesitxim-daetginreget-hoef-sfirxe-eddeogmreea-totift-ufrdeeedoof ma gartotiutunddevoefhaicglreoiunnrdeavlethimiclee. Finirsrte,apl otimntec.loFuirdst,sepgominetnctlaotuiodnsiesgamppenliteadtiotonﬁisltearpopulitendotiosefialtnedr foeuattunroeiseextarnadctifoenattuoreobetxatirnacutnioinquteo polbatnaainr anudnieqdugee fpelaatnuarers.aTnhdene, dugsiengfethateutrweso.-stTehpeLn,evuensibnegrg-tMhearqtwuaor-dsteopptiLmevizeantbioenrgm-Metahroqdu,atrhdet polpatnimarizaantdioendmgetfheoadtu, rtehse aprleanuasredantdo esdoglveefethateudreiﬀs earreenutsceodmtoposnolevnetsthoef dthifefesriexndt ecogmrepesonoefnfrtseeodfothme tsriaxndsfeogrrmeeisnotfhfereceodnotimnutoruans ssfcoarnm. AinvtehleodcnoneVtinLuPo1u6slasscearnw. Aas vuesleoddnineVthLePB16LAlaMsersywstaesmu.seFdirsitn, tthhee iBnLpAutMposyinstecmlo.uFdirdsat,tathies iﬁnlpteurtedp.oTinhtecnlo, uthdedpaotsaeitsrfainltsefroerdm. aTthioen,otfhtehpeotwseot-rfarnamsfoerpmoaintitocnlooufdthdeattwa oiscfaralcmuelatpedoibnyt tchleouGdICdPaatalgoisritchamlc,utlhaetendeabreysttphoeinGt IiCnPthealmgoarpitchomrr,estphoenndeinagretsot thpeoicnutrriennttfhreammeaips obtained according to the obtained initial pose transformation result, and GICP is performed again to\nobtain an accurate pose transformation. Finally, the point cloud data of the current frame is compared\n\nSensors 2019, 19, x FOR PEER REVIEW\n\n12 of 19\n\ncorreSsenpsoornsd20in19g, 1t9o, xthFeORcuPrErEeRnRt EfrVaImEWe is obtained according to the obtained initial pose transform1a2toiof n19\nrcomowpScrcomelcpeelouocsaniocsaecrtuubspuuuhprrouldlpreardiltprstssts,tish,i2hdi,npd,nan0eaeagao1nagagndtnh9ntndaa,d.asdiadd1slyTlogG9tiogGstno,hfothftIo2hrgIeerCt9eriCtiehmc1ihcttPt5aPephooeph.lmorictmoisdrchssuesueaepeisrpitrsrpseaacreearoeiunrntinmfnnmofdrotdortdprdeprftmifltrmnhnelrehaetategemmeeemmdfrdlpreopmeeaenoacnoamigiitaigsitnnesnaleeadietcidztciniowcnaosicmlintmltonhoitooooupbetupnthtdtaohdaohaeraberbieemnntemUrtdaUddaeaaibadinbpwmnpwuluoaianaaiantoacnthprtnchrtpueupeoat1aot1irphc6nphcdc6ceu.cgceu.0iu0bunuh4barh4lgrrailliaissisgstssst,ttyheotoyhoeaosersrenpttirdptiedthdioech.omce.amsamsTtleTholehodhebotideftsraetfrpaaatiiactio7namic7naon-oss-rs8tpeefr8torf7odorl7oeae0er0dsrnmimds0pm0nedpCeCeoitaotatenPtnteiPhntriatUridomUeedolmnidnipnpibn.ibn.ogionagnFaesFisesinleetinlowenhtowdtadcaecrchalhaalloleUloolyenlinytinuzt,bhsz,hadfttuaettoethhrthhnimrrieeoeeotmanua1anrpr2p1aplooloao6ootaobibniaof.innooo0rno1dpdpntt4e9tt soypsetreamtinogf is7y-s8t7e0m0C. PU based on the robot operating system.\n\nFigure 10. Schematic diagram of four scenes of structured corridors and mine roadways.\n4.2. ResultsFFiigguurree 1100.. SScchheemmaattiicc ddiiaaggrraamm ooff ffoouurr sscceenneess ooff ssttrruuccttuurreedd ccoorrrriiddoorrss aanndd mmiinnee rrooaaddwwaayyss.. 4.2. Results 4.2. RFeosrutlthse four experimental scenarios, the experiment was executed under the same conditions. TIaFTIaTIaMnMiMhnnhghdedUdeueUUir,Lii,nF,FneLniLEiispoopsEsp1EGrurnu1GnunGotOtattootOhht–Ooott-oeedpL-ffpp-fL.rLOftftrrtOoohohoOohAvueuevAveAirMriidaaMddaMelelelegegxaxgdddooprpoaeraraeaerairisirratrsstetheiihnthmtmthmmahhmaelaeeeenynnnpooppozaatntonenlooaalylsldylslslyyezyeezssaepcceccpcpndooedreorrdennnneendtaataddctaaaiarrnoiiciniiiccinmdntoondttisiisssospoo,,nttcanntchttho.hrhho..eeFemeFeemFdu33uu3pee.rpDDrDraxxtTathtrpphhllrheelaaeeeeaeedrssrdrrrsmee.iimsme.mmrrprooToeiieTerinnrrhcennnheeffi,efttﬁoo,e,ottrwrtchwrhhmmsemsmaeapepaeasseaeaeetftctfpieefﬀicifoieoxxoipefecnfnieecnciict.ncc.ctts.ssuuTgTTomotothmhfheeeffeaﬀdeBdeaBBpesLpssLuLupeceApeAtnnAninninMisddnsMMssgooegoe,srr,r,rrhGGGieittoiennIhfhnIfICwffCfefCefeooeoPncPrPsscrr-tmaam-t-mSiSSnmmLaiLaLaistAFesettAAiiiioMogoccMsMnsnouonhh,,nn,,,r,oLoseLsdLsdwuOwuuO1iOictntcc1AnihiAAhhoaoMn–MnaMiaaindsssns,s.,,.. Figure 11a–d.\n\nFigure 11. Cont.\n\nSensors 2019, 19, x FOR PEER REVIEW SensoSresn2so0r1s92, 01199, ,21991,5x FOR PEER REVIEW\n\n13 of 19 1133 ooff 1199\n\nFigure 11. (a) Shows the indoor scene of an office that has not been renovated and the point cloud\nFmigauprceo1n1s.tr(uac)teSdhouwsisngthteheinfdoouor rSLscAeMnemofetahnoodﬃs; c(be)ththaet ohfafsicneoctorbreidenorrescneonveaatendd athnedptohienpt colionutdclmouadp Fmcioganupsrtcerou1nc1ste.trd(uacu)tsSeidhnoguwstihsnetghfteohuienrfdoSouLorArSMsLcAemnMeetmhoofedathsn;oo(dcfs)f;isc(hbeo)twhthaset tohhﬃaesc“enriocntogrb”reidetynoprresecnueonnvdeaeatrengddroatuhnneddpthoroienaptdocwlionautydcslmcoeuandpe mcaonandpstctrhouenctspetrdouiuncttseicndlgouutshdine gmfotauhpreScfoLouAnrsMtSrLumActeMetdhombdyest;hu(ocs)dinssgh; o(fbwo)ustrhtheSeLo“AffriMicnegmc”oterytrhpidoeodursn;sdc(eednr)geirsaonuthdnedthureonapddoewirngatryoclusocnuedndemmaaanipdn ctrhooneasdptworuiancyttecsdlcoeuunsdeinmagnadtphtechoefnopsutorriunSctLtceAldoMubdymumestaihnpogdcfoson;us(rctr)SuLschAtoeMwd sbmytehutehsoi“ndrgisn;fgo(”du)rtyiSspLtehAeuMnudnmedregtrhrgooruoduns.nddromaadiwn aroyasdcwenaey asncednethaenpdotihnet pcoloinutdcmlouapd mcoanpstcrouncstetrducbtyedubsiyngusfionugrfoSuLrASMLAmMetmhoedths;od(ds). is the underground main\nrAoacdcowradyinscgentoe tahned pthoeinptocinlot ucldoumdampaopfcFoingsutrruect1e1d, bthyeufsoinugrfSoLuAr SMLAmMapmpeitnhgodms.ethods can construct a comApccleotredipnogintot cthloeupdomintapcl,owudhimchapisonfoFibgiugredi1f1fe, rtehnecfeouinr SmLaAcMro mfoarptphienginmdoeothrosdcsenceanocfotnhsetroufcfitcae cboumildpAlienctcgeopirndoiFnigtgcutlooreuthd1e1mapaoapinn,dwt chtlhoiceuhdoifsmfnicaoepbcoiogfrdFriidﬀgouerrreesnc1ce1en,ietnhomeffaFociurgorufSroLerAt1hM1ebi.mnFdaooproptrihnsecge“nmreienotghf”othdteyspocﬃeanscceceobnnuasirltdirouincogtf aiunncFodimegrupgrlreot1ue1npadoairnnodtacdthlwoeuaodyﬃ,mtcheaepcop, rowriinhdtioccrhlsoicusedneomobafipgFicdgouinfrfsetrr1eu1ncbct.eeFdionbramtsheaedc“roorinnfogtrh”etthyBepLeiAnsdMceonoaarnrsdiocetohnfeeuGonfIdCtehPreg-SrooLfuAfincMde brpouraoildpdwoinsageyd,itnhineFitpghouiisrnept c1al1poaeuradnismdraetphlaectoiovnfefslitycreucccotoemrdrpibdleaotsere.dsHcoeonwetheoevfeBFrL,igAtuhMere“a1rn1idnbg.th”FeotyGrptIehCePro-“SarLdinAwgM”aytpymrpoaepposcsceandnanirnoiotthobisfe upconanpdsetrrguirscotrueednladtbiavroseealyddwcooanmyL,pEtlheGetOep.-oLHiOnotAwcMelovueadrn, dtmhLaepO“rAcionMngs.”tTrtuhyceptseedrmobaaedtshweodadyosnmhtaahvpeecBahLniAgnhMoetrbarenedqcoutnhirseetmrGueIcCntetPsd-SobLnaAsteMhde poinrnoitLpiaEolsGpeOods-eiLn.OBtAehciMsaupasanepdtehLreOriesAirsMenla.otTiIvhMelsUye-mcliokemethpsoelendtsseo.hrHafvooerwpheoivgseheree,rstrthiemequa“tririioennmg,”etnhtytespcoeunmrotuhaledawitnivaiteyiaemlrrpaooprsoecf.atBnhneecoaptuobssee ctihos engrsretaridusucntaoeldlIyMbiUansc-elridekaeossneednL.sEAoGrtOftoh-rLepOsoaAsmeMeesattiinmmdaeLt,iOothnAe, Mtchhe. aTcruhamcetseuerliamsttieivctesheoordfrsotrhheoafvutehnedheipgrohgsreeoriusrnegqdruatdiuruenmanleleylnistnscceorneneastaherdee. iArnetiltatiahtilevpesolaysmes.emBtaeimlcla,eua,sntedhtehthceehreaferisaactnutoerreIiMsetxUictsr-laiockfteitoshneencusaonnrdnfeoortrgbpreoouspenedrefsotuirmmnnaetedilowsnc,eetlnhl.eTachureemcrueolnasttirvuecleytirorsonmroaoflflt,thhaenedppootihsnet ifcseloagturuadrdemueaaxlptlryafacintiliceordne;acfsaoenrdn.tohAtetbuethnpedeersrfagomrromeuetnidmdwem,ealtlhi.neTrhcoheaacdroawncatseytruissccteticinosen,ootfhftehtheseyuspntoedimnetrgccurlomuudnldamttiavupennfeaerilreosdrc;edfnioder atnhroet rusehnlaodtweivrglearlorygusenmfdlaumlcl,tauainantdriootanhdsewdfaueyaetsutocreennoex,tturhraencstiniyogsnt,eacmnandcnutohmteubfloeautpirveeSrfLeoArrrmoMredmidewtnheloolt.dsTshhcoaewncoclaonrnsgtserturﬂucutcictottunhaeotifcootnhmsedppuloeetientdot cnploouitnudtrcnmlionaupgd, famanildaepdt.h; efofrouthreSLuAndMermgreothuonddsmcaanincornosatdrwucatythsececnoem, pthleetesdyspteomintcculmouudlamtiavpe.error did not showIInnlartthghee feelxuxppcteeurriaimmtioeennstt pdprurooeccteeossssn,,oiinntuaarddnddiniittgii,ooannnttdoottthhheeefcocoounnrssSttLrruuAccMttiioomnn eootffhttohhdeesppcoaoniinncttoccnlloostuurdducmmt taahppe iicnnomtthhpeeleffotoeuudrr pssccoeeinnatarrciilooossu,,dththmeepaopps.oesoef tohfetehxepeerximpeernimtaleenqtaulipemqueniptminetnhte fionutrhsecefnoaurirosscisenalasroiocsalcisulaaltseod.cTahlceutlraatjeedct.oTrihese otrfatjheceIntfootrhuieersSeoLxfAptheMreimfmoeuentrhtSopLdrAsocMinesmtsh,eeitnfhooauddrsdsiicnteionthnaertiofostuharreesccseohnnoaswrtrinouscintaiorFenigsohuforewth1ne2aipn–odFin.igtucrloeu1d2am–da.p in the four scenarios, the pose of the experimental equipment in the four scenarios is also calculated. The\ntrajectories of the four SLAM methods in the four scenarios are shown in Figure 12a–d.\n\nFigure 1122.. ((aa)) SShhoowwss tthhee trajjeeccttoorryy ooff the four SLAM methods in the interior scene of the oﬃffice\nbuilding that has not been renovatedd;; (b) shows the trajectory of the four SLAM methods in the oﬃffice Fcoigrurirdeor12s.ce(na)e;S(hc)oswhsowthsethtreafjeocutrorSyLAofMthmeeftohuordsSiLnAtMhe rmuentnhiondgstrianckthoef itnhteer“iroinr gs”cetnyepeosfctehnee ooffftihcee buunidldeirnggrotuhnatdhraosandowtabye;e(ndr)esnhoovwastetdh;e(bru) nshnoinwgsttrhaecktroafjetchteorffyouorf tShLeAfoMurmSeLtAhoMdsminetthhoeduusnnindethrgeroofufincde cmoarrinidroooraasddcwwenaaeyy; s(scc)eensnheeo.. ws the four SLAM methods in the running track of the “ring” type scene of the underground roadway; (d) shows the running track of the four SLAM methods in the underground\nmain roadway scene.\n\nSensors 2019, 19, 2915\n\n14 of 19\n\nIn the indoor and underground roadway scenes, due to the experiment conditions, the actual trajectory of the equipment cannot be accurately measured. According to Figure 12, the relative positioning of the four SLAM methods in the four scenarios can be compared. For the indoor scenes of uneven ground oﬃce buildings, the positioning results of the four SLAM methods are similar, but the positioning deviation based on the LOAM method is larger than other methods; for the ﬂat oﬃce corridor scene, the positioning trajectories of the four SLAM methods are almost coincident, and the overall positioning eﬀect is better. According to the BLAM method and the GICP-SLAM method proposed in this paper, the overall positioning results are similar for the roadway scene with rough ground and inconspicuous features. However, based on the results, the LOAM and LEGO-LOAM methods have the same situation as the mapping, that is, positioning deviation occurs. The results are explained in the section on mapping results.\nIn the four scene experiments, the four SLAM methods of each scene have the same starting point and ending point. The starting position is set to the origin and there is no rotation. By analyzing the relative translation of the starting point and the ending point under the four SLAM methods of the four scenes. The relative deviations of the relative rotations are used to quantitatively analyze the positioning results of the four methods, as shown in Table 1. It can be seen from the table that the total relative translation and the total relative rotation of the four SLAM methods in the four scenarios are basically the same for the ﬁrst scenario. The positioning results of the SLAM method are consistent, but for the underground roadway scene, the deviation of the translation and rotation based on LEGO-LOAM and LOAM is also large, which also shows that it is diﬃcult to obtain accurate positioning results by relying only on the three-dimensional laser information. However, the GICP-SLAM method proposed in this paper can still achieve more accurate localization and mapping in the case of only three-dimensional laser information.\n\nTable 1. Relative deviation results of translation and rotation of the same starting point and end point under four SLAM methods in four scenarios.\n\nScenes Method Trans.1 X\n\nBlam\n\n−10.88\n\n1\n\nGicp_Slam Lego_Loam\n\n−10.04 −10.78\n\nLoam\n\n−11.20\n\nBlam\n\n−3.64\n\n2\n\nGicp_Slam Lego_Loam\n\n−3.44 −3.61\n\nLoam\n\n−3.61\n\nBlam\n\n−0.57\n\n3\n\nGicp_Slam Lego_Loam\n\n−0.01 6.05\n\nLoam\n\n3.57\n\nBlam\n\n4.09\n\n4\n\nGicp_Slam\n\n3.84\n\nLego_Loam\n\n5.70\n\nLoam\n\n2.99\n\nTrans. Y\n\nTrans. Z\n\nTotal Trans. (m)\n\nRoll\n\n−5.50\n\n0.19\n\n−5.56\n\n1.46\n\n−5.64\n\n0.14\n\n−4.23\n\n0.21\n\n12.19 11.57 12.17 11.97\n\n−0.02 −0.01 −0.16 −0.49\n\n−5.25 −5.35 −5.22 −5.23\n\n−0.12 1.45 −0.01 −0.09\n\n6.38\n\n0.00\n\n6.52\n\n0.00\n\n6.35\n\n−0.04\n\n6.35\n\n0.00\n\n9.57 8.88 14.71 8.94\n\n0.40 1.22 −3.51 −3.13\n\n9.59 8.96 16.28 10.13\n\n0.01 −0.03 −0.61 0.31\n\n73.50\n\n3.77\n\n60.04\n\n1.28\n\n73.96\n\n2.96\n\n84.75\n\n3.85\n\n73.71 60.18 74.24 84.89\n\n0.05 0.01 −0.06 −0.01\n\n1 Trans.: Translation; 2 Rotat.: Rotation.\n\nPitch\n0.05 0.03 −1.41 −1.51\n0.00 0.02 0.02 0.02\n0.03 −0.02 0.04 0.24\n−0.02 −0.03 0.15 0.19\n\nYaw\n−1.42 −1.42 −0.17 −0.50\n0.02 −0.09 −0.04 0.00\n0.46 0.37 1.14 −0.04\n0.19 0.07 0.03 0.05\n\nTotal Rotat. 2 (rad)\n1.42 1.42 1.43 1.67\n0.02 0.09 0.06 0.02\n0.46 0.37 1.29 0.39\n0.20 0.07 0.17 0.20\n\n4.3. Discussion of Results\nIn order to further analyze the localization and mapping eﬀect of the GICP-SLAM algorithm proposed in this paper. The eﬀects of the four modules of the algorithm were analyzed, licluding: point cloud registration, loop constraint, plane constraint, and running time.\n\n4.3.1. Impact of Point Cloud Registration\nThrough experimental analysis, diﬀerent point cloud registration algorithms have a greater impact on their mapping, as shown in Figure 13. Figure 13a is a point cloud registration based on NDT.\n\nSensors 2019, 19, x FOR PEER REVIEW\n\n15 of 19\n\n4.3.1. Impact of Point Cloud Registration\nSensors 2019, 19, 2915\n\n15 of 19\n\nThrough experimental analysis, different point cloud registration algorithms have a greater\n\nimpact on their mapping, as shown in Figure 13. Figure 13a is a point cloud registration based on TNhDeTp.oTinhtecploouindt creloguisdtrraetgioisntraactciuomn auclcautimveuelarrtoivreiserlarorgreisr lfarorgmerthfreormedthceirrceledcciirrcclleedciinrctlhede ﬁinguthree.fiTghuerree. Tishaerlaerigseadlaervgieatdioenviaafttieorntahfetecrotrhneecr,owrnheirc,hwrheiqcuhirreesquloiroeps dloeotpecdtieotnecftoiornfufortrhfeurrcthoerrreccotriorenc.tiFoing.uFrieg1u3rbe 1sh3bowshsothwesGthICePGbICasPedbapsoeidntpcoloinutdcrleoguidstrraetgiiosntrmataiopn. Imt caapn. IbtecsaenenbethsaetetnhethGaItCtPhereGgIiCstPratrieognisetrrraotiroins esmrroalrleisr sinmtahlelesraimn ethpeossaitmioen.pTohsietipooni.nTthcelopuodinmt acplocuodnsmtraupctceodnbstyruitchteads bleyssitdheavsialteisosnd, aenvdiattihoenp, oanindt tchloeupdoiisntmcoloreudcoinssmistoernetcwonitshisttheentlowcaitlhmtahpe loofctahlempaopinotfctlhouedp,oaisntshcloowund,inasthsheorwednciinrctlheeinreFdigcuirrcele13ibn. TFihgeuqreua1s3ib-m. Tehtheoqduahsais-ma egtrheoadt ihnaﬂsuaengcreeaotninthfleuceonncestorunctthioenceoﬀnestcrtu. cItticoannebffeescet.eInt tchaantbtheesepeonintthcaltotuhde preogiinsttrcalotiuodnrmegeitshtoradtihoansmaegtrheoadt ihnaﬂsuaegnrceeatoinnftlhueemncaepopninthgeemﬀeacptp. iTnhgeerfefegcits.trTahteiornegaicscturartaicoynbaaccsuedraocny bGaIsCePdiosnbeGtItCerP, ainsdbeitttiesrb, aetntderitadisabpetettdertoadthaeptuenddteorgthroeuunnddemrginreoupnroddmucintieonpreondvuircotinomneenntv. ironment.\n\n(a)\n\n(b)\n\nFigure 1133.. CCoommppaarrisisoonnofopf opionitnctloculodurdegriestgriasttiroantiobansebdasoend(ao)nN(oar)mNaol rDmisatlriDbuistitorinbsuTtiroansfoTrrman(sNfoDrmT) (aNndDT(b))aGndIC(Pb.) GICP.\n\n44..33..22.. IImmppaacctt ooff LLoooopp CCoonnssttrraaiinnttss TThhrroouugghh eexxppeerriimmeennttaall aannaallyyssiiss,, tthhee lloooopp ccoonnssttrraaiinntt ooff tthhee ppoossee ggrraapphh ooppttiimmiizzaattiioonn hhaass aa ggrreeaatt\niinnfﬂluueennccee oonn tthhee mmaapp ccoonnssttrruuccttiioonn,, aass sshhoowwnn iinn FFiigguurree 1144.. FFiigguurree 1144aa ddooeess nnoott aadddd lloooopp ccoonnssttrraaiinntt iinn ggrraapphh ooppttiimmiizzaattiioonn,, wwhhiicchh lleeaaddss ttoo aa llaarrggee ddeevviiaattiioonn ooff tthhee ccoonnssttrruucctteedd ppooiinntt cclloouudd mmaapp.. FFiigguurree 1144bb aaddddss lloooopp ccoonnssttrraaiinntt iinn ggrraapphh ooppttiimmiizzaattiioonn.. TThhee ccuummuullaattiivvee eerrrroorr ooff tthhee ooddoommeetteerr iiss ccoorrrreecctteedd,, aanndd fﬁinnaallllyy,, aa gglloobbaallllyyccoonnssiisstteennttppooiinnttcclloouuddmmaappiissccoonnssttrruucctteedd..\n\n(a)\n\n(b)\n\nFigure 14. EEfﬀfect of llooop ddeetection constraints on localization and mapping in pose graph. (a) WWithout loop detection, (b) with loop deteccttiioonn..\n\nSensors 2019, 19, 2915 Sensors 2019, 19, x FOR PEER REVIEW\n\n16 of 19 16 of 19\n\n4.3.3. Infﬂluence of Plane Constraints\nThroouugghheexxppeerriimmeennttaallaannaalylyssisi,s,ththeepplalnane ecocnosntsrtarianitnitninthtehpeopsoesme ampaaplsaolshoashasgaregarteiantﬂiunefnluceenocne tohnetmheapmpaipnpgirnegsurelts.uAlts. AshsoswhonwinnFinigFuirgeu1r5e.1F5i.gFuirgeu1re5a15daoedsonesotnaodt dadtdhethpelapnleandeetdeectteioctniocnoncsotnrsatirnatiinnt tihnethpeospeogsreapghra. pThh.eTchoensctornusctreudcpteodinpt ocilnout dclomuadp mcaanpnoctanbne octorbreecctoerdr,escotetdh,atsoit tihs aint ciltinisedinactlianecdertaatina acenrgtlaei.nFainggulree. 1F5igbuarded1s5ba apdladnseadpeltaencteiodnetceocntisotnracinotnsintrathinetpinostehgerpaopshe. gTrhaephc.uTmhuelactuivmeuelarrtoivreinertrhoer pinrotcheesps roofcpesossiotifopnoinsigtioanidngmaanpdpimngapispicnogrriesccteodrr,eacnteddﬁ, annadllyfianaglloybaalglylocboanllsyisctoennstisptoeinnttpcoloinutdclmouapd ims acopnistcruonctsetdru. cted.\n\n(a)\n\n(b)\n\nFigure 15. EEfﬀfect of plane detection constraints on localization and mappiinngg inn pose graphh.. (a) Without loop detection, (b) wiitthh lloooopp ddeetteeccttiioonn..\n\n44..33..44.. RRuunn TTiimmee AAnnaallyyssiiss IInn oorrddeerr ttoo eexxpplloorree tthhee rreeaall--ttiimmee ppeerrffoorrmmaannccee ooff tthhee GGIICCPP--SSLLAAMM aallggoorriitthhmm,, tthhee eexxppeerriimmeenntt wwaass\nccaarrrriieedd oouutt iinn tthhee uunnddeerrggrroouunndd rriinngg rrooaaddwwaayy ttoo aannaallyyzzee tthhee rruunnnniinngg ttiimmee ooff tthhee mmaaiinn mmoodduulleess ssuucchh aass ppllaannee ddeetteeccttiioonn,, llaasseerr ooddoommeetteerr,, lloooopp ddeetteeccttiioonn,, aanndd ggrraapphh ooppttiimmiizzaattiioonn.. SSeeeeTTaabbllee 22 ffoorr ddeettaaiillss.. AAccccoorrddiinngg ttoo tthhee ttaabbllee,, ffoorr tthhee llaasseerr ppooiinntt cclloouudd ppllaannee eexxttrraaccttiioonn,, tthhee aavveerraaggee ttiimmee ppeerr ffrraammee iiss oonnllyy aabboouutt 1100mmss,, aanndd tthheerree iiss nnoo bbiigg ddeevviiaattiioonn iinn tthhee wwhhoollee pprroocceessss.. FFoorr tthhee GGIICCPP llaasseerr ppooiinntt cclloouudd ooddoommeetteerr,, iitt ttaakkeess aabboouutt 5511 mmss ppeerr ffrraammee ttoo ppeerrffoorrmm ppooiinntt cclloouudd rreeggiissttrraattiioonn,, aanndd tthhee lloonnggeesstt nneeeeddss 111111 mmss iitteerraattiivvee mmaattcchhiinngg ttoo ccoonnvveerrggee;; SSiinnccee lloooopp ddeetteeccttiioonn rreeqquuiirreess lloooopp jjuuddggmmeenntt,, aanndd iitt iiss aallssoo nneecceessssaarryy ttoo ppeerrffoorrmm GGIICCPP--bbaasseedd rreeggiissttrraattiioonn bbeettwweeeenn lloooopp ffrraammeess ffoorr ppoossee ooppttiimmiizzaattiioonn,, ssoo tthhee aavveerraaggee lloooopp ttiimmee iiss aabboouutt 111144 mmss;; TThhee aavveerraaggiinngg ttiimmee ooff tthhee ppoossee ggrraapphh ooppttiimmiizzaattiioonn iiss aabboouutt 1155 mmss,, aanndd tthhee ccoonnvveerrggeennccee ccoonnddiittiioonn ccaann bbee qquuiicckkllyy ssaattiissﬁfieedd..\n\nTable 2. GICP-SLAM module runtime. Table 2. GICP-SLAM module runtime.\n\nModMeol del PlanParlaDnaerteDcettieocntion LiDALRLiDoOAopdRoDOmedteeoctmrtiyeotnry LoGorpapDheOtepcttiimonization Graph Optimization\n\nMaMx (amxs)(ms) Min (mMs)in (mMs)ean (msM) ean (ms)\n\n10.1130.13 121512..1011031.00 50.42152.13\n\n10.04 10.09 80.59 10.05\n\n10.04 10.09 80.59\n\n10.09 51.22 114.67 14.39\n\n10.09 51.22 114.67\n\n50.41\n\n10.05\n\n14.39\n\n5. Conclusions 5. Conclusion\nA 3D GICP-SLAM method based on underground mining environment was proposed. The rAegi3sDtraGtioICnPb-eStLwAeeMn pmoeintht ocdloubdasceodnsoencuutinvdeefrrgarmoeusn, dcomnsienciuntgiveenkveiyrofnrammeenstawndasloporpopfroasmedes. wThaes craegrriisetdraotiuotnbbaestewdeoennGpIoCinP.tIcnlnoouvdactiovneseexcutrtaivcteiofnraomf erso,acdownasyecpultaivneeskaenydfrloamopesstahnadt wloaospdefrtaemcteeds wanads ocaprtrimiedizoedutbbaasseeddoonnrGulIeCsPa.nIdnnGoIvCaPtivase gexratrpahctoiopntimofizroaatidown acyonpsltarnaienstas,nrdedlouocpinsgthtahtewcuams duelateticvteederarnodr coaputismedizbeydlbaaseserdodoonmruelteesrsaanldonGeI.CFPoar sthgeracpohnsotpruticmteidzastuiobn-mcoapns, ttrhaeinptos,inret dcluocuindgntohiesecuismauutloatmivaetiecrarlolyr rceamusoevdedbyblaasseedr oodnothmeectehrasraalcotenrei.stFiocsr tohfethceonrostarduwctaeyd. sub-map, the point cloud noise is automatically removed based on the characteristics of the roadway.\n\nSensors 2019, 19, 2915\n\n17 of 19\n\nThe proposed method was evaluated in four scenarios (such as the underground mine laboratory) and compared with the existing 3D laser SLAM method (such as LOAM). The results showed that the algorithm could realize low drift localization and point cloud map construction. This method provides technical support for real-time localization and navigation of underground mining environment.\nThe eﬀects of the main functional modules of GICP-SLAM algorithm on the localization and mapping of underground environment were analyzed from four aspects: point cloud odometer registration, loop detection, plane constraints, and system module running time. Explain the rationality of the algorithm.\nIn the subsequent work, the underground mine scene will be further combined to carry out research on multi-sensor fusion localization and mapping based on IMU, so that the positioning and mapping accuracy will be further improved. At the same time, the storage method of point cloud map and the type of data stored are studied, which is the basis for real-time localization of underground mining environments based on maps.\nAuthor Contributions: The work described in this article is the collaborative development of all authors. Data curation, Z.R.; Formal analysis, L.B.; Funding acquisition, L.W. and L.B.; Methodology, Z.R.; Project administration, L.W.; Validation, Z.R.; Writing—original draft, Z.R.; Writing—review & editing, L.B.\nFunding: This research was funded by the National Key Research and Development Plan, grant number 2017YFC0602905, and the National Natural Science Foundation of China, grant number 41572317.\nAcknowledgments: Thanks to the following organizations that provided provide basis data and technical support for this research: Changsha Digital Mine Co., Ltd.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\nReferences\n1. Liu, J.; Feng, X.; Li, Y.; Sheng, Y. Studies on temporal and spatial variation of microseismic activities in a deep metal mine. Int. J. Rock Mech. Min. Sci. 2013, 60, 171–179. [CrossRef]\n2. El Assaf, A.; Zaidi, S.; Aﬀes, S.; Kandil, N. Accurate sensors localization in underground mines or tunnels. In Proceedings of the 2015 IEEE International Conference on Ubiquitous Wireless Broadband (ICUWB), Montreal, QC, Canada, 4–7 October 2015; pp. 1–6.\n3. Kumar, S.S.; Jabannavar, S.S.; Shashank, K.R.; Nagaraj, M.; Shreenivas, B. Localization and tracking of unmanned vehicles for underground mines. In Proceedings of the 2017 Second International Conference on Electrical, Computer and Communication Technologies (ICECCT), Coimbatore, India, 22–24 February 2017; pp. 1–4.\n4. Zlot, R.; Bosse, M. Eﬃcient large-scale three-dimensional mobile mapping for underground mines. J. Field Robot. 2014, 31, 758–779. [CrossRef]\n5. Xu, Z.; Yang, W.; You, K.; Li, W.; Kim, Y. Vehicle autonomous localization in local area of coal mine tunnel based on vision sensors and ultrasonic sensors. PLoS ONE 2017, 12, e0171012. [CrossRef] [PubMed]\n6. Smith, R.; Self, M.; Cheeseman, P. Estimating uncertain spatial relationships in robotics. In Autonomous Robot Vehicles; Springer: New York, NY, USA, 1990; pp. 167–193.\n7. Huber, D.F.; Vandapel, N. Automatic three-dimensional underground mine mapping. Int. J. Robot. Res. 2006, 25, 7–17. [CrossRef]\n8. López, E.; García, S.; Barea, R.; Bergasa, L.; Molinos, E.; Arroyo, R.; Romera, E.; Pardo, S. A multi-sensorial simultaneous localization and mapping (SLAM) system for low-cost micro aerial vehicles in GPS-denied environments. Sensors 2017, 17, 802. [CrossRef] [PubMed]\n9. Bosse, M.; Zlot, R.; Flick, P. Zebedee: Design of a spring-mounted 3-d range sensor with application to mobile mapping. IEEE Trans. Robot. 2012, 28, 1104–1119. [CrossRef]\n10. Leingartner, M.; Maurer, J.; Ferrein, A.; Steinbauer, G. Evaluation of sensors and mapping approaches for disasters in tunnels. J. Field Robot. 2016, 33, 1037–1057. [CrossRef]\n11. Moosmann, F.; Stiller, C. Velodyne SLAM. In Proceedings of the 2011 IEEE Intelligent Vehicles Symposium (IV), Baden-Baden, Germany, 5–9 June 2011; pp. 393–398.\n12. Magnusson, M.; Lilienthal, A.; Duckett, T. Scan registration for autonomous mining vehicles using 3D-NDT. J. Field Robot. 2007, 24, 803–827. [CrossRef]\n\nSensors 2019, 19, 2915\n\n18 of 19\n\n13. Chen, Y.; Medioni, G. Object modelling by registration of multiple range images. Image Vis. Comput. 1992, 10, 145–155. [CrossRef]\n14. Besl, P.J.; McKay, N.D. Method for registration of 3-D shapes. Sens. Fusion IV: Control Paradig. Data Struct. 1992, 1611, 586–607. [CrossRef]\n15. Segal, A.; Haehnel, D.; Thrun, S. Generalized-ICP. In Proceedings of the Robotics: Science and Systems, Zurich, Switzerland, 25–28 June 2009; p. 435.\n16. Seraﬁn, J.; Grisetti, G. Using augmented measurements to improve the convergence of ICP. In Proceedings of the International Conference on Simulation, Modeling, and Programming for Autonomous Robots, Bergamo, Italy, 20–23 October 2014; pp. 566–577.\n17. Seraﬁn, J.; Grisetti, G. NICP: Dense normal based point cloud registration. In Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Hamburg, Germany, 28 September–2 October 2015; pp. 742–749.\n18. Censi, A.; Iocchi, L.; Grisetti, G. Scan matching in the Hough domain. In Proceedings of the 2005 IEEE International Conference on Robotics and Automation, Barcelona, Spain, 18–22 April 2005; pp. 2739–2744.\n19. Zhang, J.; Singh, S. Low-drift and real-time lidar odometry and mapping. Auton. Robot. 2017, 41, 401–416. [CrossRef]\n20. Shan, T.; Englot, B. LeGO-LOAM: Lightweight and ground-optimized lidar odometry and mapping on variable terrain. In Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Madrid, Spain, 1–5 October 2018; pp. 4758–4765.\n21. Biber, P.; Straßer, W. The normal distributions transform: A new approach to laser scan matching. In Proceedings of the 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No. 03CH37453), Las Vegas, NV, USA, 27–31 October 2003; pp. 2743–2748.\n22. Grisetti, G.; Kummerle, R.; Stachniss, C.; Burgard, W. A tutorial on graph-based SLAM. IEEE Intell. Transp. Syst. Mag. 2010, 2, 31–43. [CrossRef]\n23. Triggs, B.; McLauchlan, P.F.; Hartley, R.I.; Fitzgibbon, A.W. Bundle adjustment—A modern synthesis. In Proceedings of the International workshop on vision algorithms, Corfu, Greece, 20–25 September 1999; pp. 298–372.\n24. Latif, Y.; Cadena, C.; Neira, J. Robust loop closing over time for pose graph SLAM. Int. J. Robot. Res. 2013, 32, 1611–1626. [CrossRef]\n25. Magnusson, M.; Andreasson, H.; Nuchter, A.; Lilienthal, A.J. Appearance-based loop detection from 3D laser data using the normal distributions transform. In Proceedings of the 2009 IEEE International Conference on Robotics and Automation, Kobe, Japan, 12–17 May 2009; pp. 23–28.\n26. Sivic, J.; Zisserman, A. Eﬃcient visual search of videos cast as text retrieval. IEEE Trans. Pattern Anal. Mach. Intell. 2009, 31, 591–606. [CrossRef] [PubMed]\n27. Hess, W.; Kohler, D.; Rapp, H.; Andor, D. Real-time loop closure in 2D LIDAR SLAM. In Proceedings of the 2016 IEEE International Conference on Robotics and Automation (ICRA), Stockholm, Sweden, 16–21 May 2016; pp. 1271–1278.\n28. Ulrich, I.; Nourbakhsh, I. Appearance-based place recognition for topological localization. In Proceedings of the 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065), San Francisco, CA, USA, 24–28 April 2000; pp. 1023–1029.\n29. Schnabel, R.; Wahl, R.; Klein, R. Eﬃcient RANSAC for point-cloud shape detection. Comput. Graph. Forum 2007, 26, 214–226. [CrossRef]\n30. Ma, L.; Kerl, C.; Stückler, J.; Cremers, D. CPA-SLAM: Consistent plane-model alignment for direct RGB-D SLAM. In Proceedings of the 2016 IEEE International Conference on Robotics and Automation (ICRA), Stockholm, Sweden, 16–21 May 2016; pp. 1285–1291.\n31. Hartley, R.; Zisserman, A. Multiple View Geometry in Computer Vision; Cambridge University Press: Cambridge, UK, 2003.\n32. Kümmerle, R.; Grisetti, G.; Strasdat, H.; Konolige, K.; Burgard, W. g2o: A general framework for graph optimization. In Proceedings of the 2011 IEEE International Conference on Robotics and Automation, Shanghai, China, 9–13 May 2011; pp. 3607–3613.\n\nSensors 2019, 19, 2915\n\n19 of 19\n\n33. Hornung, A.; Wurm, K.M.; Bennewitz, M.; Stachniss, C.; Burgard, W. OctoMap: An eﬃcient probabilistic 3D mapping framework based on octrees. Auton. Robots 2013, 34, 189–206. [CrossRef]\n34. Erik-Nilson. Blam—Berkeley Localization and Mapping. Available online: https://github.com/erik-nelson/ blam (accessed on 26 June 2019).\n© 2019 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n\n"}